{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79509a1a",
   "metadata": {},
   "source": [
    "# Workbook 03b - Architectural Refinements\n",
    "\n",
    "**Objective:** Test JumpingKnowledge aggregation and attention-based global\n",
    "pooling on top of the best Wb03 configurations (GATv2 + GraphSAGE).\n",
    "\n",
    "| Modification | Options | Rationale |\n",
    "|:---|:---|:---|\n",
    "| JumpingKnowledge | none / cat / max | Leverage all layer representations |\n",
    "| GlobalAttention pool | max / attention | Learnable node importance weights |\n",
    "| LR scheduler | none / cosine | Smoother convergence |\n",
    "\n",
    "**Baselines:** GATv2 0.4964, GraphSAGE 0.4848, GLASS 0.208 (test PR-AUC).  \n",
    "**Budget:** 30-40 Optuna trials. GCN dropped (significantly underperformed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43327daf",
   "metadata": {},
   "source": [
    "## 0. Configuration and Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force non-interactive backend for script/tmux runs\n",
    "os.environ.setdefault(\"MPLBACKEND\", \"Agg\")\n",
    "import json, random, time, warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def save_fig(path, **kwargs):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\", **kwargs)\n",
    "    plt.close()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    f1_score, precision_recall_curve, confusion_matrix,\n",
    ")\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import (\n",
    "    SAGEConv, GATv2Conv,\n",
    "    global_max_pool, GlobalAttention,\n",
    "    JumpingKnowledge,\n",
    ")\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Reproducibility\n",
    "RNG_SEED = 7\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT  = Path.cwd()\n",
    "DATA_DIR      = PROJECT_ROOT / \"DATA\"\n",
    "PROCESSED     = DATA_DIR / \"processed\"\n",
    "ARRAYS_DIR    = PROCESSED / \"arrays\"\n",
    "ARTIFACTS_DIR = PROCESSED / \"artifacts\"\n",
    "PACK_DIR      = ARTIFACTS_DIR / \"packed\"\n",
    "RESULTS_DIR   = PROJECT_ROOT / \"results\"\n",
    "WB03B_DIR     = RESULTS_DIR / \"wb03b\"\n",
    "WB03B_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"  GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Experiment control\n",
    "SMOKE_TEST    = False          # True → 2 trials, 5 epochs\n",
    "N_TRIALS      = 2   if SMOKE_TEST else 35\n",
    "MAX_EPOCHS    = 5   if SMOKE_TEST else 80\n",
    "PATIENCE      = 3   if SMOKE_TEST else 15\n",
    "BATCH_SIZE    = 256\n",
    "WANDB_PROJECT = \"elliptic2-gnn\"\n",
    "WANDB_ENABLED = not SMOKE_TEST\n",
    "\n",
    "print(f\"SMOKE_TEST={SMOKE_TEST}  trials={N_TRIALS}  epochs={MAX_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    from IPython.display import display, clear_output\n",
    "except Exception:\n",
    "    display = None\n",
    "    clear_output = None\n",
    "\n",
    "def safe_display(obj):\n",
    "    if display:\n",
    "        display(obj)\n",
    "    else:\n",
    "        print(obj)\n",
    "\n",
    "def safe_clear_output():\n",
    "    if clear_output:\n",
    "        clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd8c7a",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core arrays\n",
    "X = np.load(ARRAYS_DIR / \"node_features.npy\")\n",
    "subgraph_labels = {\n",
    "    int(k): v for k, v in\n",
    "    json.loads((ARTIFACTS_DIR / \"subgraph_labels.json\").read_text()).items()\n",
    "}\n",
    "splits = json.loads((ARTIFACTS_DIR / \"splits.json\").read_text())\n",
    "\n",
    "# Packed index arrays (Wb02)\n",
    "nodes_pack        = np.load(PACK_DIR / \"nodes_by_ccid.npz\")\n",
    "edges_pack        = np.load(PACK_DIR / \"edges_by_ccid.npz\")\n",
    "unique_cc         = nodes_pack[\"unique_cc\"].astype(np.int64)\n",
    "node_ptr          = nodes_pack[\"node_ptr\"].astype(np.int64)\n",
    "node_row_perm     = nodes_pack[\"node_row_perm\"].astype(np.int64)\n",
    "unique_cc_edges   = edges_pack[\"unique_cc_edges\"].astype(np.int64)\n",
    "edge_ptr          = edges_pack[\"edge_ptr\"].astype(np.int64)\n",
    "edge_src_row_perm = edges_pack[\"edge_src_row_perm\"].astype(np.int64)\n",
    "edge_dst_row_perm = edges_pack[\"edge_dst_row_perm\"].astype(np.int64)\n",
    "\n",
    "ccid_to_i  = {int(c): i for i, c in enumerate(unique_cc)}\n",
    "ccid_to_ei = {int(c): i for i, c in enumerate(unique_cc_edges)}\n",
    "\n",
    "def label_to_int(lbl):\n",
    "    return 1 if str(lbl).lower() in {\"suspicious\", \"illicit\"} else 0\n",
    "\n",
    "y_by_cc = {int(c): label_to_int(subgraph_labels[int(c)]) for c in unique_cc}\n",
    "\n",
    "print(f\"Nodes: {X.shape[0]:,}  Features: {X.shape[1]}\")\n",
    "print(f\"Subgraphs: {len(unique_cc):,}  \"\n",
    "      f\"Suspicious: {sum(y_by_cc.values()):,} \"\n",
    "      f\"({100*sum(y_by_cc.values())/len(unique_cc):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8f717",
   "metadata": {},
   "source": [
    "## 2. Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4213537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elliptic2SubgraphDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Lazy PyG dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, ccids, make_undirected=True):\n",
    "        self.ccids = np.asarray(ccids, dtype=np.int64)\n",
    "        self.make_undirected = make_undirected\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ccids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ccid = int(self.ccids[idx])\n",
    "        i    = ccid_to_i[ccid]\n",
    "        rows = node_row_perm[node_ptr[i] : node_ptr[i + 1]]\n",
    "        x    = torch.from_numpy(X[rows]).float()\n",
    "\n",
    "        local = {int(r): j for j, r in enumerate(rows.tolist())}\n",
    "\n",
    "        if ccid in ccid_to_ei:\n",
    "            ei = ccid_to_ei[ccid]\n",
    "            s  = edge_src_row_perm[edge_ptr[ei] : edge_ptr[ei + 1]]\n",
    "            t  = edge_dst_row_perm[edge_ptr[ei] : edge_ptr[ei + 1]]\n",
    "            src = torch.tensor([local[int(r)] for r in s], dtype=torch.long)\n",
    "            dst = torch.tensor([local[int(r)] for r in t], dtype=torch.long)\n",
    "            edge_index = torch.stack([src, dst], dim=0)\n",
    "            if self.make_undirected:\n",
    "                edge_index = to_undirected(edge_index)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        y = torch.tensor([y_by_cc[ccid]], dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index, y=y, ccId=ccid)\n",
    "\n",
    "\n",
    "# Splits\n",
    "train_cc = np.array(splits[\"train\"], dtype=np.int64)\n",
    "val_cc   = np.array(splits[\"val\"],   dtype=np.int64)\n",
    "test_cc  = np.array(splits[\"test\"],  dtype=np.int64)\n",
    "\n",
    "train_ds = Elliptic2SubgraphDataset(train_cc)\n",
    "val_ds   = Elliptic2SubgraphDataset(val_cc)\n",
    "test_ds  = Elliptic2SubgraphDataset(test_cc)\n",
    "\n",
    "# Class weights (train only)\n",
    "train_labels = torch.tensor([y_by_cc[int(c)] for c in train_cc])\n",
    "n_pos = int(train_labels.sum().item())\n",
    "n_neg = int((train_labels == 0).sum().item())\n",
    "CLASS_WEIGHTS = torch.tensor(\n",
    "    [1.0, n_neg / max(n_pos, 1)], dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "IN_DIM = X.shape[1]  # 43\n",
    "\n",
    "print(f\"Class weights: licit={CLASS_WEIGHTS[0]:.1f}  suspicious={CLASS_WEIGHTS[1]:.1f}\")\n",
    "print(f\"Train: {len(train_ds):,}  Val: {len(val_ds):,}  Test: {len(test_ds):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad6cbf",
   "metadata": {},
   "source": [
    "## 3. Model Architecture - SubgraphClassifierV2\n",
    "\n",
    "Extends Wb03's `SubgraphClassifier` with:\n",
    "- **JumpingKnowledge** (`cat` / `max` / `none`)\n",
    "- **GlobalAttention** pooling (learned gate network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b74020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphClassifierV2(nn.Module):\n",
    "    \"\"\"GNN + JumpingKnowledge + attention/max pooling.\"\"\"\n",
    "\n",
    "    def __init__(self, arch, in_dim, hidden_dim=128, num_layers=2,\n",
    "                 dropout=0.1, pool=\"max\", heads=1, jk_mode=\"none\"):\n",
    "        super().__init__()\n",
    "        self.arch       = arch\n",
    "        self.dropout    = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.jk_mode    = jk_mode\n",
    "        self.pool_type  = pool\n",
    "\n",
    "        # GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            c_in = in_dim if i == 0 else hidden_dim\n",
    "\n",
    "            if arch == \"sage\":\n",
    "                self.convs.append(SAGEConv(c_in, hidden_dim))\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            elif arch == \"gatv2\":\n",
    "                if i < num_layers - 1:\n",
    "                    per_head = max(hidden_dim // heads, 1)\n",
    "                    self.convs.append(\n",
    "                        GATv2Conv(c_in, per_head, heads=heads, concat=True))\n",
    "                    actual_out = per_head * heads\n",
    "                    self.bns.append(nn.BatchNorm1d(actual_out))\n",
    "                    # override for next layer\n",
    "                    hidden_dim_next = actual_out  # noqa: F841 (used implicitly)\n",
    "                else:\n",
    "                    self.convs.append(\n",
    "                        GATv2Conv(c_in, hidden_dim, heads=1, concat=False))\n",
    "                    self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown arch: {arch}\")\n",
    "\n",
    "        # JumpingKnowledge\n",
    "        if jk_mode != \"none\":\n",
    "            self.jk = JumpingKnowledge(\n",
    "                mode=jk_mode, channels=hidden_dim, num_layers=num_layers)\n",
    "            jk_out = hidden_dim * num_layers if jk_mode == \"cat\" else hidden_dim\n",
    "        else:\n",
    "            self.jk = None\n",
    "            jk_out = hidden_dim\n",
    "\n",
    "        # Pooling\n",
    "        if pool == \"attention\":\n",
    "            gate_nn = nn.Sequential(\n",
    "                nn.Linear(jk_out, jk_out), nn.ReLU(), nn.Linear(jk_out, 1))\n",
    "            self.pool_fn = GlobalAttention(gate_nn)\n",
    "        else:\n",
    "            self.pool_fn = global_max_pool\n",
    "\n",
    "        # MLP head\n",
    "        self.lin1 = nn.Linear(jk_out, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        xs = []\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "\n",
    "        x = self.jk(xs) if self.jk is not None else xs[-1]\n",
    "\n",
    "        if self.pool_type == \"attention\":\n",
    "            g = self.pool_fn(x, batch)\n",
    "        else:\n",
    "            g = self.pool_fn(x, batch)\n",
    "\n",
    "        g = F.relu(self.lin1(g))\n",
    "        if self.dropout > 0:\n",
    "            g = F.dropout(g, p=self.dropout, training=self.training)\n",
    "        return self.lin2(g)\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "_sample = train_ds[0]\n",
    "_sample.batch = torch.zeros(_sample.x.size(0), dtype=torch.long)\n",
    "for _arch in [\"sage\", \"gatv2\"]:\n",
    "    for _jk in [\"none\", \"cat\", \"max\"]:\n",
    "        for _pool in [\"max\", \"attention\"]:\n",
    "            _m = SubgraphClassifierV2(\n",
    "                arch=_arch, in_dim=IN_DIM, hidden_dim=128,\n",
    "                num_layers=2, jk_mode=_jk, pool=_pool, heads=1)\n",
    "            with torch.no_grad():\n",
    "                _out = _m(_sample)\n",
    "            _np = sum(p.numel() for p in _m.parameters())\n",
    "            print(f\"{_arch:6s} jk={_jk:4s} pool={_pool:9s} | \"\n",
    "                  f\"params={_np:>8,} | out={tuple(_out.shape)}\")\n",
    "print(\"All model variants pass.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa887e",
   "metadata": {},
   "source": [
    "## 4. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f33cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Return (y_true, y_score) arrays.\"\"\"\n",
    "    model.eval()\n",
    "    ys, ss = [], []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        probs  = F.softmax(logits, dim=1)[:, 1]\n",
    "        ys.append(batch.y.view(-1).cpu().numpy())\n",
    "        ss.append(probs.cpu().numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ss)\n",
    "\n",
    "\n",
    "def optimal_f1_threshold(y_true, y_score):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
    "    f1 = (2 * prec * rec) / (prec + rec + 1e-12)\n",
    "    idx = int(np.nanargmax(f1))\n",
    "    return float(thr[max(idx - 1, 0)]) if len(thr) else 0.5\n",
    "\n",
    "\n",
    "def compute_test_metrics(y_true, y_score, threshold):\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return {\n",
    "        \"test_pr_auc\":    float(average_precision_score(y_true, y_score)),\n",
    "        \"test_roc_auc\":   float(roc_auc_score(y_true, y_score)),\n",
    "        \"test_f1\":        float(f1_score(y_true, y_pred)),\n",
    "        \"test_precision\": float(tp / max(tp + fp, 1)),\n",
    "        \"test_recall\":    float(tp / max(tp + fn, 1)),\n",
    "        \"test_threshold\": float(threshold),\n",
    "        \"test_confusion\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c37f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(*, arch, hidden_dim, num_layers, dropout, lr,\n",
    "                       pool, heads, jk_mode, lr_scheduler=\"none\",\n",
    "                       trial=None, verbose=False):\n",
    "    \"\"\"Train one configuration. Returns (result_dict, best_state_dict).\"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_loader = PyGDataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = PyGDataLoader(val_ds,   batch_size=512, shuffle=False)\n",
    "    test_loader  = PyGDataLoader(test_ds,  batch_size=512, shuffle=False)\n",
    "\n",
    "    model = SubgraphClassifierV2(\n",
    "        arch=arch, in_dim=IN_DIM, hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers, dropout=dropout, pool=pool,\n",
    "        heads=heads, jk_mode=jk_mode,\n",
    "    ).to(DEVICE)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = None\n",
    "    if lr_scheduler == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=MAX_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    # W&B\n",
    "    run = None\n",
    "    if WANDB_ENABLED:\n",
    "        run = wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            mode=\"online\",\n",
    "            name=f\"wb03b_{arch}_{jk_mode}_{pool}\",\n",
    "            tags=[\"wb03b\", arch, f\"jk_{jk_mode}\", f\"pool_{pool}\"],\n",
    "            config=dict(notebook=\"wb03b\", arch=arch, hidden_dim=hidden_dim,\n",
    "                        num_layers=num_layers, dropout=dropout, lr=lr,\n",
    "                        pool=pool, heads=heads, jk_mode=jk_mode,\n",
    "                        lr_scheduler=lr_scheduler, n_params=n_params),\n",
    "            reinit=True)\n",
    "\n",
    "    best_val_pr, best_state, best_epoch, bad_epochs = -1.0, None, 0, 0\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch), batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch.num_graphs\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "\n",
    "        cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        yv, sv = evaluate(model, val_loader)\n",
    "        vp = float(average_precision_score(yv, sv))\n",
    "        vr = float(roc_auc_score(yv, sv))\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"  epoch {epoch:3d}  loss={avg_loss:.4f}\"\n",
    "                  f\"  val_pr={vp:.4f}  val_roc={vr:.4f}\")\n",
    "\n",
    "        if run:\n",
    "            wandb.log({\"train/loss\": avg_loss, \"val/pr_auc\": vp,\n",
    "                        \"val/roc_auc\": vr, \"lr\": cur_lr, \"epoch\": epoch})\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(vp, epoch)\n",
    "            if trial.should_prune():\n",
    "                if run:\n",
    "                    wandb.log({\"pruned\": True}); run.finish(quiet=True)\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        if vp > best_val_pr + 1e-4:\n",
    "            best_val_pr = vp\n",
    "            best_state  = {k: v.detach().cpu().clone()\n",
    "                           for k, v in model.state_dict().items()}\n",
    "            best_epoch  = epoch\n",
    "            bad_epochs  = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    wall = time.time() - t0\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    yt, st = evaluate(model, test_loader)\n",
    "    thr    = optimal_f1_threshold(yt, st)\n",
    "    tm     = compute_test_metrics(yt, st, thr)\n",
    "\n",
    "    result = dict(arch=arch, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                  dropout=dropout, lr=lr, pool=pool, heads=heads,\n",
    "                  jk_mode=jk_mode, lr_scheduler=lr_scheduler,\n",
    "                  n_params=n_params, best_val_pr_auc=best_val_pr,\n",
    "                  best_epoch=best_epoch, wall_seconds=wall, **tm)\n",
    "\n",
    "    if run:\n",
    "        wandb.log(dict(best_val_pr_auc=best_val_pr, best_epoch=best_epoch,\n",
    "                        n_params=n_params, wall_seconds=wall, **tm))\n",
    "        run.finish(quiet=True)\n",
    "\n",
    "    return result, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac074f",
   "metadata": {},
   "source": [
    "## 5. Optuna Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35726464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective():\n",
    "    \"\"\"Optuna objective searching across SAGE + GATv2 with JK/attention.\"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        arch       = trial.suggest_categorical(\"arch\",       [\"sage\", \"gatv2\"])\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256])\n",
    "        num_layers = trial.suggest_categorical(\"num_layers\", [2, 3])\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.05, 0.30, step=0.05)\n",
    "        lr         = trial.suggest_float(\"lr\", 5e-4, 3e-3, log=True)\n",
    "        jk_mode    = trial.suggest_categorical(\"jk_mode\",       [\"none\", \"cat\", \"max\"])\n",
    "        pool       = trial.suggest_categorical(\"pool\",          [\"max\", \"attention\"])\n",
    "        lr_sched   = trial.suggest_categorical(\"lr_scheduler\",  [\"none\", \"cosine\"])\n",
    "\n",
    "        heads = 1\n",
    "        if arch == \"gatv2\":\n",
    "            heads = trial.suggest_categorical(\"heads\", [1, 2])\n",
    "\n",
    "        result, _ = train_and_evaluate(\n",
    "            arch=arch, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            dropout=dropout, lr=lr, pool=pool, heads=heads,\n",
    "            jk_mode=jk_mode, lr_scheduler=lr_sched, trial=trial)\n",
    "\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3cce5",
   "metadata": {},
   "source": [
    "### Execute Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"wb03b_arch_refinements\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    ")\n",
    "study.optimize(make_objective(), n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "all_results = [t.user_attrs[\"result\"]\n",
    "               for t in study.trials\n",
    "               if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "n_pruned = sum(1 for t in study.trials\n",
    "               if t.state == optuna.trial.TrialState.PRUNED)\n",
    "print(f\"\\nCompleted: {len(all_results)}/{N_TRIALS}  \"\n",
    "      f\"Pruned: {n_pruned} ({100*n_pruned/max(N_TRIALS,1):.0f}%)\")\n",
    "print(f\"Best trial #{study.best_trial.number}  val PR-AUC = {study.best_value:.4f}\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "(WB03B_DIR / \"search_results.json\").write_text(json.dumps(all_results, indent=2))\n",
    "print(f\"Saved {len(all_results)} results → {WB03B_DIR / 'search_results.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf906a",
   "metadata": {},
   "source": [
    "## 6. Controlled Ablation Studies\n",
    "\n",
    "Isolate each modification against the Wb03 best GATv2 baseline:\n",
    "\n",
    "| Name | JK | Pool | What it tests |\n",
    "|:---|:---|:---|:---|\n",
    "| wb03_repro | none | max | Wb03 reproduction |\n",
    "| attention_only | none | attention | Attention pooling effect |\n",
    "| jk_cat_only | cat | max | JumpingKnowledge effect |\n",
    "| both | cat | attention | Combined effect |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c43468",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = dict(arch=\"gatv2\", hidden_dim=256, num_layers=2,\n",
    "            dropout=0.20, lr=0.00075, heads=1)\n",
    "\n",
    "ablations = [\n",
    "    (\"wb03_repro\",      dict(jk_mode=\"none\", pool=\"max\",       lr_scheduler=\"none\")),\n",
    "    (\"attention_only\",  dict(jk_mode=\"none\", pool=\"attention\", lr_scheduler=\"none\")),\n",
    "    (\"jk_cat_only\",     dict(jk_mode=\"cat\",  pool=\"max\",       lr_scheduler=\"none\")),\n",
    "    (\"both_jk_att\",     dict(jk_mode=\"cat\",  pool=\"attention\", lr_scheduler=\"none\")),\n",
    "]\n",
    "\n",
    "abl_results = []\n",
    "for name, overrides in ablations:\n",
    "    print(f\"\\n{'='*60}\\n  {name}\\n{'='*60}\")\n",
    "    cfg = {**BASE, **overrides}\n",
    "    res, _ = train_and_evaluate(**cfg, verbose=True)\n",
    "    res[\"ablation_name\"] = name\n",
    "    abl_results.append(res)\n",
    "    print(f\"  Val PR-AUC:  {res['best_val_pr_auc']:.4f}\")\n",
    "    print(f\"  Test PR-AUC: {res['test_pr_auc']:.4f}  F1: {res['test_f1']:.4f}\")\n",
    "\n",
    "(WB03B_DIR / \"ablation_results.json\").write_text(json.dumps(abl_results, indent=2))\n",
    "print(f\"\\nSaved {len(abl_results)} ablations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459e0fb",
   "metadata": {},
   "source": [
    "## 7. Results Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ca357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results).sort_values(\"test_pr_auc\", ascending=False)\n",
    "cols = [\"arch\", \"hidden_dim\", \"num_layers\", \"jk_mode\", \"pool\", \"lr_scheduler\",\n",
    "        \"dropout\", \"lr\", \"best_val_pr_auc\", \"test_pr_auc\", \"test_roc_auc\",\n",
    "        \"test_f1\", \"test_precision\", \"test_recall\", \"n_params\",\n",
    "        \"best_epoch\", \"wall_seconds\"]\n",
    "cols = [c for c in cols if c in df.columns]\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"ALL COMPLETED TRIALS (by test PR-AUC)\")\n",
    "print(\"=\"*90)\n",
    "print(df[cols].head(20).to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "best = df.iloc[0]\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"BEST: {best['arch']} jk={best['jk_mode']} pool={best['pool']}\")\n",
    "print(f\"  Test PR-AUC: {best['test_pr_auc']:.4f}  (Wb03 best: 0.4964)\")\n",
    "print(f\"  ROC-AUC: {best['test_roc_auc']:.4f}  F1: {best['test_f1']:.4f}\")\n",
    "print(f\"  Prec: {best['test_precision']:.4f}  Rec: {best['test_recall']:.4f}\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "print(\"\\nABLATION COMPARISON:\")\n",
    "adf = pd.DataFrame(abl_results)\n",
    "print(adf[[\"ablation_name\",\"jk_mode\",\"pool\",\"best_val_pr_auc\",\n",
    "           \"test_pr_auc\",\"test_f1\",\"n_params\"]].to_string(\n",
    "               index=False, float_format=\"%.4f\"))\n",
    "\n",
    "print(\"\\nBASELINES:\")\n",
    "for nm, v in [(\"Random\", 0.023), (\"LogReg (Wb02)\", 0.154),\n",
    "              (\"GLASS (2024)\", 0.208), (\"SAGE tuned (Wb03)\", 0.4848),\n",
    "              (\"GATv2 tuned (Wb03)\", 0.4964),\n",
    "              (f\"Best Wb03b\", float(best['test_pr_auc']))]:\n",
    "    print(f\"  {nm:30s} {v:.4f}  {'█' * int(v * 100)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72da04d",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = df.iloc[0].to_dict()\n",
    "print(\"Re-training best configuration for checkpoint save...\")\n",
    "res_best, state_best = train_and_evaluate(\n",
    "    arch=best_cfg[\"arch\"],\n",
    "    hidden_dim=int(best_cfg[\"hidden_dim\"]),\n",
    "    num_layers=int(best_cfg[\"num_layers\"]),\n",
    "    dropout=float(best_cfg[\"dropout\"]),\n",
    "    lr=float(best_cfg[\"lr\"]),\n",
    "    pool=best_cfg[\"pool\"],\n",
    "    heads=int(best_cfg.get(\"heads\", 1)),\n",
    "    jk_mode=best_cfg[\"jk_mode\"],\n",
    "    lr_scheduler=best_cfg.get(\"lr_scheduler\", \"none\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": state_best,\n",
    "    \"config\": {k: best_cfg[k] for k in\n",
    "               [\"arch\",\"hidden_dim\",\"num_layers\",\"dropout\",\"lr\",\n",
    "                \"pool\",\"heads\",\"jk_mode\",\"lr_scheduler\"]\n",
    "               if k in best_cfg},\n",
    "    \"metrics\": {k: res_best[k] for k in\n",
    "                [\"best_val_pr_auc\",\"test_pr_auc\",\"test_roc_auc\",\n",
    "                 \"test_f1\",\"test_precision\",\"test_recall\"]},\n",
    "    \"in_dim\": IN_DIM,\n",
    "}, WB03B_DIR / \"best_model.pt\")\n",
    "print(f\"Saved → {WB03B_DIR / 'best_model.pt'}  PR-AUC={res_best['test_pr_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11bd69",
   "metadata": {},
   "source": [
    "## 9. Optuna Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "vals = [t.value for t in study.trials if t.value is not None]\n",
    "axes[0].plot(vals, \"o-\", ms=4, alpha=0.7)\n",
    "axes[0].axhline(study.best_value, color=\"red\", ls=\"--\", alpha=0.5,\n",
    "                label=f\"Best: {study.best_value:.4f}\")\n",
    "axes[0].set(xlabel=\"Trial\", ylabel=\"Val PR-AUC\", title=\"Optimisation History\")\n",
    "axes[0].legend()\n",
    "\n",
    "try:\n",
    "    imp = optuna.importance.get_param_importances(study)\n",
    "    ps, vs = list(imp.keys())[:8], [imp[p] for p in list(imp.keys())[:8]]\n",
    "    axes[1].barh(ps[::-1], vs[::-1])\n",
    "    axes[1].set(xlabel=\"Importance\", title=\"Parameter Importance (fANOVA)\")\n",
    "except Exception as e:\n",
    "    axes[1].text(0.5, 0.5, f\"Error: {e}\", ha=\"center\", va=\"center\",\n",
    "                 transform=axes[1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(WB03B_DIR / \"optuna_diagnostics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "save_fig(\"results/fig_01.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a43ae",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Outputs saved to `results/wb03b/`:\n",
    "- `search_results.json` - all completed trial results\n",
    "- `ablation_results.json` - four controlled ablations\n",
    "- `best_model.pt` - checkpoint of best model\n",
    "- `optuna_diagnostics.png` - search visualisation\n",
    "\n",
    "Results feed into Wb03c2 (if attention pooling wins, use as default)\n",
    "and the explainability chapter (attention weights provide interpretability).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}