{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6d58c0",
   "metadata": {},
   "source": [
    "# Elliptic2 Data Pre-processing\n",
    "**Research workflow, reproducible artifacts, and auditable quality checks**\n",
    "\n",
    "\n",
    "## Abstract\n",
    "This notebook implements a reproducible pre-processing pipeline for the Elliptic2 dataset for subgraph-level classification. The workflow is structured to be auditable, reproducible (fixed seed, persisted splits, mappings, and train-only statistics), and efficient for iteration (CSV→Parquet conversion and cached subsets).\n",
    "\n",
    "---\n",
    "\n",
    "### Objectivesof notebook\n",
    "Given the raw Elliptic2 dataset files, we will implement a pre-processing pipeline to prepare:\n",
    "- **Parquet copies** of key tables (fast reads, stable dtypes)\n",
    "- **Node re-indexing** (map raw IDs → contiguous 0..N-1)\n",
    "- **Subgraph membership mapping** (component/subgraph id per node)\n",
    "- **Sanity checks**:\n",
    "  - edges reference existing nodes\n",
    "  - labels consistent within subgraphs\n",
    "  - no accidental leakage when scaling\n",
    "- **Feature statistics** computed on training set only\n",
    "- **Train/val/test splits** at the **subgraph level**\n",
    "- **Model-ready artifacts** saved to `processed/`\n",
    "\n",
    "\n",
    "\n",
    "### Inputs (expected files)\n",
    "Required:\n",
    "- `nodes.csv`: labeled nodes participating in subgraphs\n",
    "- `edges.csv`: edges restricted to labeled subgraphs\n",
    "- `connected_components.csv`: mapping from node identifier to component/subgraph identifier\n",
    "- `background_nodes.csv`, `background_edges.csv`: full background graph; used only for advanced context augmentation (not required for initial baselines)\n",
    "\n",
    "\n",
    "### Outputs (written to `processed/`)\n",
    "- `processed/parquet/*.parquet`: Parquet copies of the core tables\n",
    "- `processed/arrays/node_features.npy`: node feature matrix aligned to `nodes.csv` row order\n",
    "- `processed/arrays/edge_index.npy`: 2×E edge index in contiguous node index space\n",
    "- `processed/arrays/node_components.npy`: component/subgraph id per node aligned to `nodes.csv`\n",
    "- `processed/artifacts/node_index.json`: mapping raw node_id → contiguous idx\n",
    "- `processed/artifacts/feature_columns.json`: ordered list of feature columns used\n",
    "- `processed/artifacts/subgraph_labels.json`: mapping subgraph_id → label\n",
    "- `processed/artifacts/splits.json`: persisted train/val/test split ids\n",
    "- `processed/artifacts/feature_stats_train.json`: training-only feature statistics\n",
    "\n",
    "---\n",
    "\n",
    "All random processes (splitting) are controlled by a fixed RNG seed. Split definitions are serialized to disk and should be reused for all experiments to ensure comparability across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6680693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "print(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210fbbbd",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Everything written by this notebook goes under `processed/`.\n",
    "\n",
    "Set `DATA_DIR` to the folder containing your CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265fcf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present: ['nodes', 'edges', 'cc', 'bg_nodes', 'bg_edges']\n"
     ]
    }
   ],
   "source": [
    "# Update this path to your local Elliptic2 directory\n",
    "DATA_DIR = Path(\"DATA\")\n",
    "\n",
    "OUT_DIR = Path(\"DATA/processed\")\n",
    "PARQUET_DIR = OUT_DIR / \"parquet\"\n",
    "ARTIFACTS_DIR = OUT_DIR / \"artifacts\"\n",
    "ARRAYS_DIR = OUT_DIR / \"arrays\"\n",
    "\n",
    "for d in [OUT_DIR, PARQUET_DIR, ARTIFACTS_DIR, ARRAYS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Expected filenames\n",
    "FILES = {\n",
    "    \"nodes\": DATA_DIR / \"nodes.csv\",\n",
    "    \"edges\": DATA_DIR / \"edges.csv\",\n",
    "    \"cc\": DATA_DIR / \"connected_components.csv\",\n",
    "    \"bg_nodes\": DATA_DIR / \"background_nodes.csv\",\n",
    "    \"bg_edges\": DATA_DIR / \"background_edges.csv\",\n",
    "}\n",
    "\n",
    "present = {k: p for k, p in FILES.items() if p.exists()}\n",
    "missing = {k: str(p) for k, p in FILES.items() if not p.exists()}\n",
    "\n",
    "print(\"Present:\", list(present.keys()))\n",
    "if missing:\n",
    "    print(\"Missing (optional where applicable):\")\n",
    "    for k, p in missing.items():\n",
    "        print(f\" - {k}: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a042e3",
   "metadata": {},
   "source": [
    "## 2. Lightweight schema introspection\n",
    "\n",
    "Lets inspect the first few rows and check:\n",
    "- node identifier column\n",
    "- edge endpoint columns (source/target)\n",
    "- component/subgraph identifier column\n",
    "- label column (if stored at node-level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986905a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clId</th>\n",
       "      <th>ccId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515498410</td>\n",
       "      <td>41121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630366534</td>\n",
       "      <td>27974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>903790945</td>\n",
       "      <td>108020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>449108887</td>\n",
       "      <td>6544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>877994419</td>\n",
       "      <td>27234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        clId    ccId\n",
       "0  515498410   41121\n",
       "1  630366534   27974\n",
       "2  903790945  108020\n",
       "3  449108887    6544\n",
       "4  877994419   27234"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clId1</th>\n",
       "      <th>clId2</th>\n",
       "      <th>txId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753456251</td>\n",
       "      <td>753456254</td>\n",
       "      <td>29911377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>756183927</td>\n",
       "      <td>759736869</td>\n",
       "      <td>51855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>623574254</td>\n",
       "      <td>622935561</td>\n",
       "      <td>27784128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751464959</td>\n",
       "      <td>751464964</td>\n",
       "      <td>76668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751464834</td>\n",
       "      <td>751464959</td>\n",
       "      <td>2592471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clId1      clId2      txId\n",
       "0  753456251  753456254  29911377\n",
       "1  756183927  759736869     51855\n",
       "2  623574254  622935561  27784128\n",
       "3  751464959  751464964     76668\n",
       "4  751464834  751464959   2592471"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ccId</th>\n",
       "      <th>ccLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>licit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>licit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>licit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>licit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>licit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ccId ccLabel\n",
       "0     0   licit\n",
       "1     1   licit\n",
       "2     2   licit\n",
       "3     3   licit\n",
       "4     4   licit"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_csv_head(path: Path, nrows: int = 5) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, nrows=nrows, low_memory=False)\n",
    "\n",
    "nodes_head = read_csv_head(present[\"nodes\"])\n",
    "edges_head = read_csv_head(present[\"edges\"])\n",
    "cc_head = read_csv_head(present[\"cc\"])\n",
    "\n",
    "display(nodes_head.head())\n",
    "display(edges_head.head())\n",
    "display(cc_head.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85e337",
   "metadata": {},
   "source": [
    "## 3. Infer key columns\n",
    "\n",
    "### Rationale\n",
    "For this Elliptic2 release we can see the main columns are named as follows:\n",
    "\n",
    "- `nodes.csv` contains node membership: clId (node id) and ccId (subgraph id)\n",
    "\n",
    "- `edges.csv` contains topology: clId1 and clId2 (edge endpoints), with txId as an optional edge identifier\n",
    "\n",
    "- `connected_components.csv` contains supervision: ccId and ccLabel (one label per subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b062d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred columns:\n",
      " - nodes.csv: node_id=clId, component_id=ccId\n",
      " - edges.csv: src=clId1, dst=clId2\n",
      " - connected_components.csv: component_id=ccId, label=ccLabel\n"
     ]
    }
   ],
   "source": [
    "NODE_ID_COL      = \"clId\"     # nodes.csv\n",
    "NODES_COMP_COL   = \"ccId\"     # nodes.csv\n",
    "\n",
    "SRC_COL          = \"clId1\"    # edges.csv\n",
    "DST_COL          = \"clId2\"    # edges.csv\n",
    "TX_ID_COL        = \"txId\"     # edges.csv \n",
    "\n",
    "CC_COMP_COL      = \"ccId\"     # connected_components.csv\n",
    "CC_LABEL_COL     = \"ccLabel\"  # connected_components.csv\n",
    "\n",
    "def require_columns(df: pd.DataFrame, required: list[str], table_name: str) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{table_name} is missing expected columns: {missing}\\n\"\n",
    "            f\"Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "require_columns(nodes_head, [NODE_ID_COL, NODES_COMP_COL], \"nodes.csv\")\n",
    "require_columns(edges_head, [SRC_COL, DST_COL], \"edges.csv\")\n",
    "\n",
    "# this is not a necessity so we can just warn\n",
    "if TX_ID_COL not in edges_head.columns:\n",
    "    print(f\"Note: {TX_ID_COL} not found in edges.csv (ok if you don't use edge attributes).\")\n",
    "\n",
    "require_columns(cc_head, [CC_COMP_COL, CC_LABEL_COL], \"connected_components.csv\")\n",
    "\n",
    "# Cross-table sanity\n",
    "if NODES_COMP_COL != CC_COMP_COL:\n",
    "    raise ValueError(\n",
    "        f\"Component column mismatch: nodes uses '{NODES_COMP_COL}' but connected_components uses '{CC_COMP_COL}'.\"\n",
    "    )\n",
    "\n",
    "print(\"Inferred columns:\")\n",
    "print(f\" - nodes.csv: node_id={NODE_ID_COL}, component_id={NODES_COMP_COL}\")\n",
    "print(f\" - edges.csv: src={SRC_COL}, dst={DST_COL}, tx_id={TX_ID_COL if TX_ID_COL in edges_head.columns else '(none)'}\")\n",
    "print(f\" - connected_components.csv: component_id={CC_COMP_COL}, label={CC_LABEL_COL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d91a84",
   "metadata": {},
   "source": [
    "## 4. Full table ingestion with dtype controls\n",
    "\n",
    "We will:\n",
    "- load the full tables\n",
    "- coerce ID columns to integers (after validating missingness)\n",
    "- keep feature columns as numeric types when possible\n",
    "\n",
    "If coercion fails, we stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711671d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: {'nodes': (444521, 2), 'edges': (367137, 3), 'cc': (121810, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clId    int64\n",
       "ccId    int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clId1    int64\n",
       "clId2    int64\n",
       "txId     int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ccId                int64\n",
       "ccLabel    string[python]\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_csv_full(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "nodes = read_csv_full(present[\"nodes\"])\n",
    "edges = read_csv_full(present[\"edges\"])\n",
    "cc = read_csv_full(present[\"cc\"])\n",
    "\n",
    "print(\"Shapes:\", {\"nodes\": nodes.shape, \"edges\": edges.shape, \"cc\": cc.shape})\n",
    "\n",
    "# Enforce non-null for identity columns\n",
    "identity_checks = [\n",
    "    (\"nodes\", nodes, NODE_ID_COL),        # clId\n",
    "    (\"nodes\", nodes, NODES_COMP_COL),     # ccId\n",
    "    (\"edges\", edges, SRC_COL),            # clId1\n",
    "    (\"edges\", edges, DST_COL),            # clId2\n",
    "    (\"cc\", cc, CC_COMP_COL),              # ccId\n",
    "    (\"cc\", cc, CC_LABEL_COL),             # ccLabel\n",
    "]\n",
    "for name, df, col in identity_checks:\n",
    "    na = int(df[col].isna().sum())\n",
    "    if na:\n",
    "        raise ValueError(\n",
    "            f\"Non-null violation: {name}.{col} contains {na} missing values. \"\n",
    "            \"Fix upstream before proceeding.\"\n",
    "        )\n",
    "\n",
    "# Enforce integer identifiers\n",
    "nodes[NODE_ID_COL] = nodes[NODE_ID_COL].astype(\"int64\")\n",
    "nodes[NODES_COMP_COL] = nodes[NODES_COMP_COL].astype(\"int64\")\n",
    "\n",
    "edges[SRC_COL] = edges[SRC_COL].astype(\"int64\")\n",
    "edges[DST_COL] = edges[DST_COL].astype(\"int64\")\n",
    "\n",
    "cc[CC_COMP_COL] = cc[CC_COMP_COL].astype(\"int64\")\n",
    "# ccLabel is categorical (strings like 'licit', 'illicit', 'unknown'); do NOT cast to int\n",
    "cc[CC_LABEL_COL] = cc[CC_LABEL_COL].astype(\"string\")\n",
    "\n",
    "display(nodes.dtypes)\n",
    "display(edges.dtypes)\n",
    "display(cc.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4016df0",
   "metadata": {},
   "source": [
    "## 5. SHA256 file fingerprints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4553b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\processed\\artifacts\\input_fingerprints.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nodes': {'path': 'DATA\\\\nodes.csv',\n",
       "  'partial_sha256_first5MB': '1fc3f79a211e2544f21490f4b4d9a616314f7f64c3e3478189fc678589e12063'},\n",
       " 'edges': {'path': 'DATA\\\\edges.csv',\n",
       "  'partial_sha256_first5MB': 'c026e367ae38ab6a46806d877c9a98310f0fc28a5bb9e1bef1c050d93d55ca10'},\n",
       " 'cc': {'path': 'DATA\\\\connected_components.csv',\n",
       "  'partial_sha256_first5MB': '79511c49f76110d0feae7feda7e85279bc442efe31f5e7a9f53b2caf0019062b'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partial_sha256(path: Path, nbytes: int = 5_000_000) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        h.update(f.read(nbytes))\n",
    "    return h.hexdigest()\n",
    "\n",
    "fingerprints = {}\n",
    "for k in [\"nodes\", \"edges\", \"cc\"]:\n",
    "    p = present[k]\n",
    "    fingerprints[k] = {\"path\": str(p), \"partial_sha256_first5MB\": partial_sha256(p)}\n",
    "\n",
    "(ARTIFACTS_DIR / \"input_fingerprints.json\").write_text(json.dumps(fingerprints, indent=2))\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"input_fingerprints.json\")\n",
    "fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6272b1",
   "metadata": {},
   "source": [
    "## 6. Parquet conversion \n",
    "\n",
    "Parquet benefits:\n",
    "- **Faster** reads (columnar)\n",
    "- **Smaller** on disk (compression)\n",
    "- **Stable dtypes** across sessions\n",
    "- Easy to select subsets of columns without reading the entire file\n",
    "\n",
    "We save Parquet copies to `processed/parquet/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1125d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes.parquet: 4.4 MB\n",
      "edges.parquet: 7.0 MB\n",
      "connected_components.parquet: 0.8 MB\n"
     ]
    }
   ],
   "source": [
    "nodes.to_parquet(PARQUET_DIR / \"nodes.parquet\", index=False)\n",
    "edges.to_parquet(PARQUET_DIR / \"edges.parquet\", index=False)\n",
    "cc.to_parquet(PARQUET_DIR / \"connected_components.parquet\", index=False)\n",
    "\n",
    "for p in [PARQUET_DIR / \"nodes.parquet\", PARQUET_DIR / \"edges.parquet\", PARQUET_DIR / \"connected_components.parquet\"]:\n",
    "    print(f\"{p.name}: {p.stat().st_size/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b7f04",
   "metadata": {},
   "source": [
    "## 7. Graph integrity checks\n",
    "We validate:\n",
    "- Every `src` and `dst` exists in `nodes.csv`.\n",
    "- Every node in `nodes.csv` has a component ID (if `connected_components.csv` is meant to cover all labeled nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown endpoints: {'unknown_src': 0, 'unknown_dst': 0}\n",
      "Nodes whose ccId is missing in connected_components.csv: 0\n",
      "Integrity checks passed.\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Referential integrity: edges to nodes\n",
    "node_set = set(nodes[NODE_ID_COL].tolist())  \n",
    "unknown_src = int((~edges[SRC_COL].isin(node_set)).sum()) \n",
    "unknown_dst = int((~edges[DST_COL].isin(node_set)).sum())  \n",
    "\n",
    "print(\"Unknown endpoints:\", {\"unknown_src\": unknown_src, \"unknown_dst\": unknown_dst})\n",
    "\n",
    "if unknown_src or unknown_dst:\n",
    "    bad = edges.loc[\n",
    "        (~edges[SRC_COL].isin(node_set)) | (~edges[DST_COL].isin(node_set)),\n",
    "        [SRC_COL, DST_COL]\n",
    "    ].head(20)\n",
    "    raise ValueError(\n",
    "        \"Referential integrity failure: edges reference clIds not present in nodes.csv. Example:\\n\"\n",
    "        + str(bad)\n",
    "    )\n",
    "\n",
    "# 7.2 Component completeness: nodes ids all mentioned in connected components\n",
    "ccid_set = set(cc[CC_COMP_COL].tolist())\n",
    "missing_ccid = int((~nodes[NODES_COMP_COL].isin(ccid_set)).sum())  \n",
    "print(\"Nodes whose ccId is missing in connected_components.csv:\", missing_ccid)\n",
    "\n",
    "if missing_ccid:\n",
    "    ex = nodes.loc[~nodes[NODES_COMP_COL].isin(ccid_set), [NODE_ID_COL, NODES_COMP_COL]].head(20)\n",
    "    raise ValueError(\n",
    "        \"Component completeness failure: some nodes have ccId values not present in connected_components.csv. Example:\\n\"\n",
    "        + str(ex)\n",
    "    )\n",
    "\n",
    "print(\"Integrity checks passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a9298",
   "metadata": {},
   "source": [
    "## 8. Contiguous node re-indexing\n",
    "\n",
    "#Raw node IDs are huge and sparse. For model inputs we want:\n",
    "- compact integer range: `0..N-1`\n",
    "- stable mapping saved to disk\n",
    "- fast conversion of edge lists to `edge_index` arrays\n",
    "\n",
    "We build:\n",
    "- `node_ids_sorted`: array of raw IDs\n",
    "- `node2idx`: dict mapping raw ID → compact index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18ad2106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\processed\\artifacts\\node_index.json\n",
      "Saved: DATA\\processed\\arrays\\edge_index.npy shape: (2, 367137)\n"
     ]
    }
   ],
   "source": [
    "node_ids_sorted = np.sort(nodes[NODE_ID_COL].to_numpy(dtype=np.int64))\n",
    "node2idx: Dict[int, int] = {int(nid): int(i) for i, nid in enumerate(node_ids_sorted)}\n",
    "\n",
    "(ARTIFACTS_DIR / \"node_index.json\").write_text(json.dumps(node2idx))\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"node_index.json\")\n",
    "\n",
    "src_idx = edges[SRC_COL].map(node2idx).to_numpy(dtype=np.int64)\n",
    "dst_idx = edges[DST_COL].map(node2idx).to_numpy(dtype=np.int64)\n",
    "\n",
    "edge_index = np.vstack([src_idx, dst_idx])\n",
    "assert edge_index.min() >= 0 and edge_index.max() < len(node_ids_sorted)\n",
    "\n",
    "np.save(ARRAYS_DIR / \"edge_index.npy\", edge_index)\n",
    "print(\"Saved:\", ARRAYS_DIR / \"edge_index.npy\", \"shape:\", edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f54ab4",
   "metadata": {},
   "source": [
    "## 9. Subgraph membership per node\n",
    "\n",
    "ML libraries represent edges as a 2×E integer matrix:\n",
    "- first row: sources\n",
    "- second row: targets\n",
    "\n",
    "We can map raw IDs through `node2idx` and then validate the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a932152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\processed\\arrays\\node_components.npy\n",
      "Unique components: 121810\n"
     ]
    }
   ],
   "source": [
    "# Elliptic2: nodes.csv already contains ccId per node row\n",
    "node_components = nodes[NODES_COMP_COL].to_numpy(dtype=np.int64)  # NODES_COMP_COL = \"ccId\"\n",
    "\n",
    "np.save(ARRAYS_DIR / \"node_components.npy\", node_components)\n",
    "print(\"Saved:\", ARRAYS_DIR / \"node_components.npy\")\n",
    "print(\"Unique components:\", int(np.unique(node_components).shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d634d",
   "metadata": {},
   "source": [
    "## 10. Label integrity \n",
    "\n",
    "We assert that for each component id, there is exactly one unique label.\n",
    "\n",
    "Then we derive:\n",
    "- `subgraph_labels`: dict {component_id: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fefed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping size (cc): 121810\n",
      "Components used by nodes: 121810\n",
      "Components labeled but unused by nodes: 0\n",
      "Saved: DATA\\processed\\artifacts\\subgraph_labels.json\n"
     ]
    }
   ],
   "source": [
    "# Build subgraph label mapping directly from connected_components.csv\n",
    "subgraph_labels = cc.set_index(CC_COMP_COL)[CC_LABEL_COL].to_dict()  # ccId -> \"licit\"/\"illicit\"/\"unknown\"\n",
    "\n",
    "# Audit: every component used by nodes must have a label row\n",
    "comps = node_components\n",
    "used_ccids = set(np.unique(comps).astype(int).tolist())\n",
    "labeled_ccids = set(cc[CC_COMP_COL].astype(int).tolist())\n",
    "\n",
    "missing_labels = sorted(list(used_ccids - labeled_ccids))\n",
    "if missing_labels:\n",
    "    print(\"Components present in nodes.csv but missing in connected_components.csv:\", len(missing_labels))\n",
    "    print(\"Examples:\", missing_labels[:20])\n",
    "    raise ValueError(\n",
    "        \"Label integrity check failed: some ccIds used by nodes have no ccLabel entry. \"\n",
    "        \"Verify connected_components.csv completeness.\"\n",
    "    )\n",
    "\n",
    "# Audit: connected_components may contain ccIds not used by nodes\n",
    "extra_labels = sorted(list(labeled_ccids - used_ccids))\n",
    "print(\"Label mapping size (cc):\", len(subgraph_labels))\n",
    "print(\"Components used by nodes:\", len(used_ccids))\n",
    "print(\"Components labeled but unused by nodes:\", len(extra_labels))\n",
    "\n",
    "(ARTIFACTS_DIR / \"subgraph_labels.json\").write_text(json.dumps(subgraph_labels))\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"subgraph_labels.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020d438",
   "metadata": {},
   "source": [
    "## 11. Feature matrix construction and missingness policy\n",
    "\n",
    "Elliptic2 are frequently **binned** (ordinal categories). This affects preprocessing:\n",
    "- You must preserve them as integers where possible.\n",
    "- Missing values should be represented as a dedicated bin (e.g., 0 or max+1).\n",
    "- Converting to float and leaving NaNs causes:\n",
    "  - slow models\n",
    "  - broken embeddings\n",
    "  - inconsistent scaling\n",
    "\n",
    "We will:\n",
    "- define `feature_cols` as all columns excluding ID and label\n",
    "- compute missingness\n",
    "- choose a missing value policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22d2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clId</th>\n",
       "      <th>feat#1</th>\n",
       "      <th>feat#2</th>\n",
       "      <th>feat#3</th>\n",
       "      <th>feat#4</th>\n",
       "      <th>feat#5</th>\n",
       "      <th>feat#6</th>\n",
       "      <th>feat#7</th>\n",
       "      <th>feat#8</th>\n",
       "      <th>feat#9</th>\n",
       "      <th>feat#10</th>\n",
       "      <th>feat#11</th>\n",
       "      <th>feat#12</th>\n",
       "      <th>feat#13</th>\n",
       "      <th>feat#14</th>\n",
       "      <th>feat#15</th>\n",
       "      <th>feat#16</th>\n",
       "      <th>feat#17</th>\n",
       "      <th>feat#18</th>\n",
       "      <th>feat#19</th>\n",
       "      <th>feat#20</th>\n",
       "      <th>feat#21</th>\n",
       "      <th>feat#22</th>\n",
       "      <th>feat#23</th>\n",
       "      <th>feat#24</th>\n",
       "      <th>feat#25</th>\n",
       "      <th>feat#26</th>\n",
       "      <th>feat#27</th>\n",
       "      <th>feat#28</th>\n",
       "      <th>feat#29</th>\n",
       "      <th>feat#30</th>\n",
       "      <th>feat#31</th>\n",
       "      <th>feat#32</th>\n",
       "      <th>feat#33</th>\n",
       "      <th>feat#34</th>\n",
       "      <th>feat#35</th>\n",
       "      <th>feat#36</th>\n",
       "      <th>feat#37</th>\n",
       "      <th>feat#38</th>\n",
       "      <th>feat#39</th>\n",
       "      <th>feat#40</th>\n",
       "      <th>feat#41</th>\n",
       "      <th>feat#42</th>\n",
       "      <th>feat#43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>284528470</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>65</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>91</td>\n",
       "      <td>42</td>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>84</td>\n",
       "      <td>68</td>\n",
       "      <td>82</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>563997622</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1577338</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>87</td>\n",
       "      <td>80</td>\n",
       "      <td>96</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>54</td>\n",
       "      <td>85</td>\n",
       "      <td>46</td>\n",
       "      <td>73</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>51</td>\n",
       "      <td>82</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>87</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>725905686</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96159393</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>53</td>\n",
       "      <td>88</td>\n",
       "      <td>53</td>\n",
       "      <td>81</td>\n",
       "      <td>18</td>\n",
       "      <td>53</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "      <td>62</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>66</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        clId  feat#1  feat#2  feat#3  feat#4  feat#5  feat#6  feat#7  feat#8  feat#9  feat#10  feat#11  feat#12  feat#13  feat#14  feat#15  feat#16  feat#17  \\\n",
       "0  284528470      75      68      65      56      56      57      91      42      69       44       69       35       51        5       17       13       16   \n",
       "1  563997622      41      25       0       4       4       0      50      35       0       37        0        0        0        7        0       15        0   \n",
       "2    1577338      92      91      88      89      87      80      96      52      85       54       85       46       73       17       54       24       51   \n",
       "3  725905686      50      36       0       0       6       0      17      46       0       47        0        0        0        1        0       10        0   \n",
       "4   96159393      99      99      99      99      96      89      93      51      88       53       88       53       81       18       53       24       48   \n",
       "\n",
       "   feat#18  feat#19  feat#20  feat#21  feat#22  feat#23  feat#24  feat#25  feat#26  feat#27  feat#28  feat#29  feat#30  feat#31  feat#32  feat#33  feat#34  \\\n",
       "0       72       84       68       82        7       85       62       67        2        1        1        4        3        6       59       66        8   \n",
       "1       67        0       38       44        7       48       35       32        1        0        0        0        0        0       50       47        9   \n",
       "2       82       89       66       80        5       87       66       75        2        2        2        4        3        5       74       85        5   \n",
       "3       44        0       56        0        7        0       47        0        2        0        1        0        0        0       62        0        9   \n",
       "4       83       90       62       80        3       99       59       70        1        2        1        4        2        4       73       85        7   \n",
       "\n",
       "   feat#35  feat#36  feat#37  feat#38  feat#39  feat#40  feat#41  feat#42  feat#43  \n",
       "0       85       58       49        2        1        0        1        0        0  \n",
       "1       42       39       26        1        0        0        0        0        0  \n",
       "2       90       70       69        2        2        2        5        2        3  \n",
       "3        0       45        0        2        0        0        0        0        0  \n",
       "4       90       66       64        2        1        4        6        2        4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Polars streaming semi-join (fast path).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neric\\AppData\\Local\\Temp\\ipykernel_26888\\4146292525.py:55: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars subset rows: 444521\n",
      "Cached subset written to: DATA\\processed\\parquet\\background_nodes_subset.parquet (rows=444521)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat#1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat#20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         missing_count  missing_rate\n",
       "feat#1               0           0.0\n",
       "feat#2               0           0.0\n",
       "feat#3               0           0.0\n",
       "feat#4               0           0.0\n",
       "feat#5               0           0.0\n",
       "feat#6               0           0.0\n",
       "feat#7               0           0.0\n",
       "feat#8               0           0.0\n",
       "feat#9               0           0.0\n",
       "feat#10              0           0.0\n",
       "feat#11              0           0.0\n",
       "feat#12              0           0.0\n",
       "feat#13              0           0.0\n",
       "feat#14              0           0.0\n",
       "feat#15              0           0.0\n",
       "feat#16              0           0.0\n",
       "feat#17              0           0.0\n",
       "feat#18              0           0.0\n",
       "feat#19              0           0.0\n",
       "feat#20              0           0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\processed\\artifacts\\feature_columns.json\n",
      "Saved: DATA\\processed\\arrays\\node_features.npy shape: (444521, 43)\n",
      "Feature dtype summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int8    43\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Elliptic2 feature construction (node features are in background_nodes.csv) ---\n",
    "# Fast path: Polars (streaming semi-join)\n",
    "# Fallback: Pandas chunking with progress-based ETA\n",
    "# Outputs:\n",
    "# - processed/parquet/background_nodes_subset.parquet\n",
    "# - processed/artifacts/feature_columns.json\n",
    "# - processed/arrays/node_features.npy\n",
    "\n",
    "import os, time, json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Identify labeled node universe\n",
    "labeled_clids = nodes[NODE_ID_COL].astype(\"int64\").tolist()\n",
    "labeled_clid_set = set(labeled_clids)\n",
    "\n",
    "# 2) Ensure background_nodes.csv exists\n",
    "if \"bg_nodes\" not in present:\n",
    "    raise FileNotFoundError(\n",
    "        \"background_nodes.csv not found. Elliptic2 node features live in background_nodes.csv, \"\n",
    "        \"so you need this file to build node_features.npy.\"\n",
    "    )\n",
    "\n",
    "bg_nodes_path = present[\"bg_nodes\"]\n",
    "\n",
    "# 3) Determine ID column in background_nodes.csv (usually clId)\n",
    "bg_head = pd.read_csv(bg_nodes_path, nrows=5, low_memory=False)\n",
    "display(bg_head.head())\n",
    "\n",
    "BG_NODE_ID_COL = \"clId\"\n",
    "if BG_NODE_ID_COL not in bg_head.columns:\n",
    "    raise ValueError(\n",
    "        f\"Expected '{BG_NODE_ID_COL}' in background_nodes.csv but found: {bg_head.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "# 4) If we already cached the subset as Parquet\n",
    "bg_subset_parquet = PARQUET_DIR / \"background_nodes_subset.parquet\"\n",
    "if bg_subset_parquet.exists():\n",
    "    print(f\"Loading cached subset: {bg_subset_parquet}\")\n",
    "    bg_subset = pd.read_parquet(bg_subset_parquet)\n",
    "else:\n",
    "    # 5) Create subset (try Polars, else Pandas)\n",
    "    try:\n",
    "        import polars as pl\n",
    "\n",
    "        print(\"Using Polars streaming semi-join (fast path).\")\n",
    "\n",
    "        # Semi-join keeps only rows whose clId appears in labeled set\n",
    "        labeled_lazy = pl.DataFrame({BG_NODE_ID_COL: labeled_clids}).lazy()\n",
    "\n",
    "        bg_subset_pl = (\n",
    "            pl.scan_csv(str(bg_nodes_path))\n",
    "              .join(labeled_lazy, on=BG_NODE_ID_COL, how=\"semi\")\n",
    "              .collect(streaming=True)\n",
    "        )\n",
    "\n",
    "        bg_subset = bg_subset_pl.to_pandas()\n",
    "        print(\"Polars subset rows:\", len(bg_subset))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Polars path unavailable or failed; falling back to Pandas chunking.\")\n",
    "        print(\"Reason:\", repr(e))\n",
    "\n",
    "        chunk_size = 1_000_000  # tune based on RAM/SSD; 1M is usually safe\n",
    "        file_size = os.path.getsize(bg_nodes_path)\n",
    "\n",
    "        kept = []\n",
    "        bytes_est = 0\n",
    "        start = time.time()\n",
    "        last_print = 0.0\n",
    "\n",
    "        reader = pd.read_csv(bg_nodes_path, chunksize=chunk_size, low_memory=False)\n",
    "\n",
    "        for chunk in tqdm(reader, desc=\"Subsetting background_nodes (pandas)\"):\n",
    "            # approximate progress using chunk memory footprint\n",
    "            bytes_est += int(chunk.memory_usage(deep=True).sum())\n",
    "\n",
    "            # ensure ID dtype is comparable\n",
    "            chunk[BG_NODE_ID_COL] = chunk[BG_NODE_ID_COL].astype(\"int64\", copy=False)\n",
    "\n",
    "            sub = chunk[chunk[BG_NODE_ID_COL].isin(labeled_clid_set)]\n",
    "            if not sub.empty:\n",
    "                kept.append(sub)\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            if elapsed - last_print >= 10:  # print every ~10s\n",
    "                rate = bytes_est / max(elapsed, 1e-9)  # bytes/sec (approx)\n",
    "                eta_sec = (file_size / max(rate, 1e-9)) - elapsed\n",
    "                print(f\"[ETA] ~{eta_sec/60:.1f} min remaining | ~{rate/1e6:.1f} MB/s (approx)\")\n",
    "                last_print = elapsed\n",
    "\n",
    "        if not kept:\n",
    "            raise ValueError(\"No rows in background_nodes.csv matched labeled clIds. Check ID columns/types.\")\n",
    "        bg_subset = pd.concat(kept, ignore_index=True)\n",
    "\n",
    "    # 6) Cache subset to Parquet \n",
    "    bg_subset.to_parquet(bg_subset_parquet, index=False)\n",
    "    print(f\"Cached subset written to: {bg_subset_parquet} (rows={len(bg_subset)})\")\n",
    "\n",
    "# 7) Define feature columns (exclude the id column)\n",
    "feature_cols = [c for c in bg_subset.columns if c != BG_NODE_ID_COL]\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(\"No feature columns found in background_nodes subset (only ID column present).\")\n",
    "\n",
    "# 8) Missingness diagnostics (on subset)\n",
    "missing_counts = bg_subset[feature_cols].isna().sum().sort_values(ascending=False)\n",
    "missing_rate = (missing_counts / len(bg_subset)).sort_values(ascending=False)\n",
    "diagnostics = pd.DataFrame({\"missing_count\": missing_counts, \"missing_rate\": missing_rate})\n",
    "display(diagnostics.head(20))\n",
    "\n",
    "# 9) Missingness policy (binned/ordinal features → fill with reserved bin)\n",
    "MISSING_FILL_VALUE = 0\n",
    "features_df = bg_subset[feature_cols].copy().fillna(MISSING_FILL_VALUE)\n",
    "\n",
    "# Conservative integer coercion / downcast\n",
    "for c in feature_cols:\n",
    "    if pd.api.types.is_float_dtype(features_df[c]):\n",
    "        as_int = features_df[c].astype(np.int64)\n",
    "        if np.allclose(features_df[c].to_numpy(), as_int.to_numpy()):\n",
    "            features_df[c] = as_int\n",
    "    if pd.api.types.is_integer_dtype(features_df[c]):\n",
    "        features_df[c] = pd.to_numeric(features_df[c], downcast=\"integer\")\n",
    "\n",
    "# 10) Align features to nodes.csv row order\n",
    "# Create mapping clId -> feature row (expect 1-to-1 for background_nodes)\n",
    "bg_subset_indexed = pd.concat([bg_subset[[BG_NODE_ID_COL]], features_df], axis=1).set_index(BG_NODE_ID_COL)\n",
    "\n",
    "# Assert all labeled nodes have background features\n",
    "missing_feat = [cl for cl in labeled_clids if cl not in bg_subset_indexed.index]\n",
    "if missing_feat:\n",
    "    print(\"Labeled nodes missing background features:\", len(missing_feat))\n",
    "    print(\"Examples:\", missing_feat[:20])\n",
    "    raise ValueError(\n",
    "        \"Some labeled clIds have no row in background_nodes.csv (after subsetting). \"\n",
    "        \"Verify that background_nodes.csv covers the same clId universe.\"\n",
    "    )\n",
    "\n",
    "# Build X aligned to nodes.csv row order (same row order as nodes)\n",
    "X = bg_subset_indexed.loc[labeled_clids, feature_cols].to_numpy()\n",
    "\n",
    "# 11) Persist artifacts\n",
    "(ARTIFACTS_DIR / \"feature_columns.json\").write_text(json.dumps(feature_cols))\n",
    "np.save(ARRAYS_DIR / \"node_features.npy\", X)\n",
    "\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"feature_columns.json\")\n",
    "print(\"Saved:\", ARRAYS_DIR / \"node_features.npy\", \"shape:\", X.shape)\n",
    "print(\"Feature dtype summary:\")\n",
    "display(bg_subset_indexed[feature_cols].dtypes.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecbd20",
   "metadata": {},
   "source": [
    "## 12. Subgraph-level stratified splits (train/validation/test)\n",
    "\n",
    "1. **Correct unit of evaluation**  \n",
    "   Each subgraph is one training instance. Splitting at node-level would leak subgraph structure across splits.\n",
    "\n",
    "2. **Class imbalance**  \n",
    "   Elliptic2 has strong imbalance between licit and suspicious. Stratification ensures each split contains\n",
    "   representative class proportions.\n",
    "\n",
    "We will:\n",
    "- build a dataframe of one row per subgraph: (subgraph_id, label)\n",
    "- stratified split into train/test\n",
    "- stratified split train into train/val\n",
    "- save `splits.json` with lists of subgraph IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb280e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "licit         119047\n",
       "suspicious      2763\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\processed\\artifacts\\splits.json\n",
      "{'train': 85267, 'val': 12181, 'test': 24362}\n"
     ]
    }
   ],
   "source": [
    "subgraph_df = pd.DataFrame({\n",
    "    \"subgraph_id\": list(subgraph_labels.keys()),\n",
    "    \"label\": list(subgraph_labels.values()),\n",
    "}).sort_values(\"subgraph_id\").reset_index(drop=True)\n",
    "\n",
    "display(subgraph_df[\"label\"].value_counts())\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RNG_SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(subgraph_df[\"subgraph_id\"], subgraph_df[\"label\"]))\n",
    "\n",
    "trainval = subgraph_df.iloc[trainval_idx].reset_index(drop=True)\n",
    "test = subgraph_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.125, random_state=RNG_SEED)\n",
    "train_idx, val_idx = next(sss2.split(trainval[\"subgraph_id\"], trainval[\"label\"]))\n",
    "\n",
    "train = trainval.iloc[train_idx].reset_index(drop=True)\n",
    "val = trainval.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "splits = {\n",
    "    \"seed\": RNG_SEED,\n",
    "    \"train\": train[\"subgraph_id\"].astype(int).tolist(),\n",
    "    \"val\": val[\"subgraph_id\"].astype(int).tolist(),\n",
    "    \"test\": test[\"subgraph_id\"].astype(int).tolist(),\n",
    "}\n",
    "\n",
    "(ARTIFACTS_DIR / \"splits.json\").write_text(json.dumps(splits))\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"splits.json\")\n",
    "print({k: len(v) for k, v in splits.items() if k in {\"train\",\"val\",\"test\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27662e04",
   "metadata": {},
   "source": [
    "## 13. Training-only feature statistics (leakage-safe diagnostics)\n",
    "\n",
    "Any scaling parameters must be computed on **training data only** to avoid leakage.\n",
    "\n",
    "We compute per-feature:\n",
    "- min, max\n",
    "- mean, std\n",
    "- unique count (useful for binned/categorical features)\n",
    "- missing count (after fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dea35091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node count: 310253\n",
      "Saved: DATA\\processed\\artifacts\\feature_stats_train.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feat#19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>18.971697</td>\n",
       "      <td>30.509784</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feat#23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>18.516163</td>\n",
       "      <td>27.171481</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feat#35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>8.538167</td>\n",
       "      <td>22.252337</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feat#20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>44.941996</td>\n",
       "      <td>13.192262</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feat#7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>34.971736</td>\n",
       "      <td>25.411256</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feat#21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>22.506574</td>\n",
       "      <td>28.853333</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feat#11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>12.985434</td>\n",
       "      <td>20.246851</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feat#9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>12.993818</td>\n",
       "      <td>20.262075</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feat#25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>16.362424</td>\n",
       "      <td>21.951214</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>feat#33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>6.868768</td>\n",
       "      <td>16.919134</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feat#8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>30.335478</td>\n",
       "      <td>9.223867</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>feat#12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.172888</td>\n",
       "      <td>6.977590</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature  min   max       mean        std  unique\n",
       "0   feat#19  0.0  98.0  18.971697  30.509784      98\n",
       "1   feat#23  0.0  96.0  18.516163  27.171481      97\n",
       "2   feat#35  0.0  96.0   8.538167  22.252337      96\n",
       "3   feat#20  3.0  96.0  44.941996  13.192262      94\n",
       "4    feat#7  0.0  98.0  34.971736  25.411256      91\n",
       "5   feat#21  0.0  94.0  22.506574  28.853333      91\n",
       "6   feat#11  0.0  93.0  12.985434  20.246851      90\n",
       "7    feat#9  0.0  93.0  12.993818  20.262075      89\n",
       "8   feat#25  0.0  86.0  16.362424  21.951214      87\n",
       "9   feat#33  0.0  89.0   6.868768  16.919134      85\n",
       "10   feat#8  0.0  82.0  30.335478   9.223867      82\n",
       "11  feat#12  0.0  84.0   1.172888   6.977590      82"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sg = set(splits[\"train\"])\n",
    "train_mask = np.array([int(comp) in train_sg for comp in comps], dtype=bool)\n",
    "\n",
    "X_train = X[train_mask]\n",
    "print(\"Training node count:\", X_train.shape[0])\n",
    "\n",
    "feature_stats = {}\n",
    "for j, col in enumerate(feature_cols):\n",
    "    col_data = np.asarray(X_train[:, j])\n",
    "    feature_stats[col] = {\n",
    "        \"min\": float(np.nanmin(col_data)),\n",
    "        \"max\": float(np.nanmax(col_data)),\n",
    "        \"mean\": float(np.nanmean(col_data)),\n",
    "        \"std\": float(np.nanstd(col_data)),\n",
    "        \"unique\": int(len(np.unique(col_data))),\n",
    "    }\n",
    "\n",
    "(ARTIFACTS_DIR / \"feature_stats_train.json\").write_text(json.dumps(feature_stats))\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"feature_stats_train.json\")\n",
    "\n",
    "# Display top-cardinality features (often informative for embeddings vs scaling decisions)\n",
    "top_unique = sorted(feature_stats.items(), key=lambda kv: kv[1][\"unique\"], reverse=True)[:12]\n",
    "display(pd.DataFrame([{**{\"feature\": k}, **v} for k, v in top_unique]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42aa793",
   "metadata": {},
   "source": [
    "## Summary of produced artifacts\n",
    "\n",
    "### Artifacts produced\n",
    "\n",
    "- `arrays/edge_index.npy`: graph topology in contiguous index space\n",
    "- `arrays/node_components.npy`: ccId membership per node (aligned to nodes.csv)\n",
    "- `arrays/node_features.npy`: node feature matrix X aligned to labelled clIds\n",
    "- `artifacts/subgraph_labels.json`: ccId → ccLabel mapping\n",
    "- `artifacts/splits.json`: persisted train/val/test ccId partitions\n",
    "- `artifacts/feature_stats_train.json`: train-only feature diagnostics\n",
    "\n",
    "\n",
    "### What is now validated\n",
    "\n",
    "- The dataset is explicitly subgraph-labelled (ccLabel at component level), so the prediction unit is ccId.\n",
    "- Structural integrity holds for the labelled subset: edges are closed over labelled nodes and component/label coverage is complete.\n",
    "- Feature coverage is complete for labelled nodes via background subsetting, producing a consistent (444521, 43) node-feature matrix.\n",
    "\n",
    "### Immediate implications for modelling\n",
    "Class imbalance is severe (~2.2% suspicious by component), so evaluation should prioritise PR-AUC / minority-class F1 and use stratified, component-level splits.\n",
    "\n",
    "Features are discretised/binned integers; embeddings (per-feature or per-bin) are a natural baseline to compare against scaling-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198faba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tud-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
