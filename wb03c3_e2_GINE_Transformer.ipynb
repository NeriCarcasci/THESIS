{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34311612",
   "metadata": {},
   "source": [
    "# Workbook 03c3 — Lighter Edge Feature Architectures\n",
    "\n",
    "**Motivation:** NNConv (Wb03c2) overfits on tiny subgraphs because it generates\n",
    "a full weight matrix per edge (95 → in×out). We test two lighter alternatives\n",
    "that incorporate edge features without per-edge weight matrices:\n",
    "\n",
    "| Architecture | How it uses edge features | Why it suits tiny subgraphs |\n",
    "|:---|:---|:---|\n",
    "| **GINEConv** | Adds edge features as a bias after the message MLP | Very few extra params; hard to overfit |\n",
    "| **TransformerConv** | Projects edge features into attention scores | Edge-aware attention like GATv2 but with edge signal |\n",
    "\n",
    "**Baselines:**\n",
    "- GATv2 (Wb03, no edge features): 0.4964 test PR-AUC\n",
    "- NNConv best (Wb03c2): whatever it produced (likely lower)\n",
    "\n",
    "**Budget:** 20 trials per architecture (40 total), plus dense-feature variants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba81246",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"MPLBACKEND\", \"Agg\")\n",
    "import json, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_fig(path, **kwargs):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\", **kwargs)\n",
    "    plt.close()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    f1_score, precision_recall_curve, confusion_matrix,\n",
    ")\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv, TransformerConv,\n",
    "    global_max_pool, GlobalAttention, JumpingKnowledge,\n",
    ")\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "RNG_SEED = 7\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "PROJECT_ROOT  = Path.cwd()\n",
    "DATA_DIR      = PROJECT_ROOT / \"DATA\"\n",
    "PROCESSED     = DATA_DIR / \"processed\"\n",
    "ARRAYS_DIR    = PROCESSED / \"arrays\"\n",
    "ARTIFACTS_DIR = PROCESSED / \"artifacts\"\n",
    "PACK_DIR      = ARTIFACTS_DIR / \"packed\"\n",
    "RESULTS_DIR   = PROJECT_ROOT / \"results\"\n",
    "WB03C3_DIR    = RESULTS_DIR / \"wb03c3\"\n",
    "WB03C3_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"  GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "SMOKE_TEST    = False\n",
    "N_TRIALS      = 2  if SMOKE_TEST else 20\n",
    "MAX_EPOCHS    = 5  if SMOKE_TEST else 80\n",
    "PATIENCE      = 3  if SMOKE_TEST else 15\n",
    "BATCH_SIZE    = 256\n",
    "WANDB_PROJECT = \"elliptic2-gnn\"\n",
    "WANDB_ENABLED = not SMOKE_TEST\n",
    "\n",
    "print(f\"SMOKE_TEST={SMOKE_TEST}  trials={N_TRIALS}  epochs={MAX_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5a4dd",
   "metadata": {},
   "source": [
    "## 1. Load Data + Edge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(ARRAYS_DIR / \"node_features.npy\")\n",
    "subgraph_labels = {\n",
    "    int(k): v for k, v in\n",
    "    json.loads((ARTIFACTS_DIR / \"subgraph_labels.json\").read_text()).items()\n",
    "}\n",
    "splits = json.loads((ARTIFACTS_DIR / \"splits.json\").read_text())\n",
    "\n",
    "nodes_pack        = np.load(PACK_DIR / \"nodes_by_ccid.npz\")\n",
    "edges_pack        = np.load(PACK_DIR / \"edges_by_ccid.npz\")\n",
    "unique_cc         = nodes_pack[\"unique_cc\"].astype(np.int64)\n",
    "node_ptr          = nodes_pack[\"node_ptr\"].astype(np.int64)\n",
    "node_row_perm     = nodes_pack[\"node_row_perm\"].astype(np.int64)\n",
    "unique_cc_edges   = edges_pack[\"unique_cc_edges\"].astype(np.int64)\n",
    "edge_ptr          = edges_pack[\"edge_ptr\"].astype(np.int64)\n",
    "edge_src_row_perm = edges_pack[\"edge_src_row_perm\"].astype(np.int64)\n",
    "edge_dst_row_perm = edges_pack[\"edge_dst_row_perm\"].astype(np.int64)\n",
    "\n",
    "ccid_to_i  = {int(c): i for i, c in enumerate(unique_cc)}\n",
    "ccid_to_ei = {int(c): i for i, c in enumerate(unique_cc_edges)}\n",
    "\n",
    "def label_to_int(lbl):\n",
    "    return 1 if str(lbl).lower() in {\"suspicious\", \"illicit\"} else 0\n",
    "\n",
    "y_by_cc = {int(c): label_to_int(subgraph_labels[int(c)]) for c in unique_cc}\n",
    "\n",
    "# Edge features (from Wb03c1)\n",
    "EDGE_FEATURES = np.load(ARRAYS_DIR / \"edge_features.npy\")\n",
    "EDGE_FEAT_DIM = EDGE_FEATURES.shape[1]\n",
    "\n",
    "# Dense subset (from Wb03c1)\n",
    "EDGE_FEATURES_DENSE = np.load(ARRAYS_DIR / \"edge_features_dense.npy\")\n",
    "DENSE_FEAT_DIM = EDGE_FEATURES_DENSE.shape[1]\n",
    "dense_meta = json.loads((ARTIFACTS_DIR / \"edge_feature_dense_meta.json\").read_text())\n",
    "\n",
    "IN_DIM = X.shape[1]  # 43\n",
    "\n",
    "print(f\"Node features: {X.shape}\")\n",
    "print(f\"Edge features: {EDGE_FEATURES.shape} (all), {EDGE_FEATURES_DENSE.shape} (dense)\")\n",
    "print(f\"Subgraphs: {len(unique_cc):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756b459",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f44e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elliptic2EdgeFeatureDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyG dataset with edge features. Identical to Wb03c2.\"\"\"\n",
    "\n",
    "    def __init__(self, ccids, edge_features=None, use_random_edges=False):\n",
    "        self.ccids = np.asarray(ccids, dtype=np.int64)\n",
    "        self.edge_features = edge_features if edge_features is not None else EDGE_FEATURES\n",
    "        self.use_random_edges = use_random_edges\n",
    "        self._rng = np.random.RandomState(RNG_SEED)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ccids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ccid = int(self.ccids[idx])\n",
    "        i    = ccid_to_i[ccid]\n",
    "        rows = node_row_perm[node_ptr[i] : node_ptr[i + 1]]\n",
    "        x    = torch.from_numpy(X[rows]).float()\n",
    "\n",
    "        local = {int(r): j for j, r in enumerate(rows.tolist())}\n",
    "\n",
    "        if ccid in ccid_to_ei:\n",
    "            ei = ccid_to_ei[ccid]\n",
    "            s  = edge_src_row_perm[edge_ptr[ei] : edge_ptr[ei + 1]]\n",
    "            t  = edge_dst_row_perm[edge_ptr[ei] : edge_ptr[ei + 1]]\n",
    "\n",
    "            global_edge_start = edge_ptr[ei]\n",
    "            global_edge_end   = edge_ptr[ei + 1]\n",
    "\n",
    "            src = torch.tensor([local[int(r)] for r in s], dtype=torch.long)\n",
    "            dst = torch.tensor([local[int(r)] for r in t], dtype=torch.long)\n",
    "\n",
    "            edge_index_dir = torch.stack([src, dst], dim=0)\n",
    "\n",
    "            e_feats = self.edge_features[global_edge_start:global_edge_end]\n",
    "            if self.use_random_edges:\n",
    "                e_feats = self._rng.randn(*e_feats.shape).astype(np.float32)\n",
    "            edge_attr_dir = torch.from_numpy(e_feats.copy()).float()\n",
    "\n",
    "            edge_index_rev = torch.stack([dst, src], dim=0)\n",
    "            edge_index = torch.cat([edge_index_dir, edge_index_rev], dim=1)\n",
    "            edge_attr  = torch.cat([edge_attr_dir, edge_attr_dir], dim=0)\n",
    "\n",
    "            from torch_geometric.utils import coalesce\n",
    "            edge_index, edge_attr = coalesce(\n",
    "                edge_index, edge_attr, reduce=\"mean\")\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr  = torch.empty((0, self.edge_features.shape[1]),\n",
    "                                     dtype=torch.float32)\n",
    "\n",
    "        y = torch.tensor([y_by_cc[ccid]], dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                    y=y, ccId=ccid)\n",
    "\n",
    "\n",
    "train_cc = np.array(splits[\"train\"], dtype=np.int64)\n",
    "val_cc   = np.array(splits[\"val\"],   dtype=np.int64)\n",
    "test_cc  = np.array(splits[\"test\"],  dtype=np.int64)\n",
    "\n",
    "train_ds = Elliptic2EdgeFeatureDataset(train_cc)\n",
    "val_ds   = Elliptic2EdgeFeatureDataset(val_cc)\n",
    "test_ds  = Elliptic2EdgeFeatureDataset(test_cc)\n",
    "\n",
    "train_ds_dense = Elliptic2EdgeFeatureDataset(train_cc, edge_features=EDGE_FEATURES_DENSE)\n",
    "val_ds_dense   = Elliptic2EdgeFeatureDataset(val_cc,   edge_features=EDGE_FEATURES_DENSE)\n",
    "test_ds_dense  = Elliptic2EdgeFeatureDataset(test_cc,  edge_features=EDGE_FEATURES_DENSE)\n",
    "\n",
    "train_labels = torch.tensor([y_by_cc[int(c)] for c in train_cc])\n",
    "n_pos = int(train_labels.sum().item())\n",
    "n_neg = int((train_labels == 0).sum().item())\n",
    "CLASS_WEIGHTS = torch.tensor(\n",
    "    [1.0, n_neg / max(n_pos, 1)], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "print(f\"Class weights: {CLASS_WEIGHTS.tolist()}\")\n",
    "print(f\"Sample: {train_ds[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee175148",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "### GINEConv\n",
    "GIN (Graph Isomorphism Network) with edge features. The edge attr is added\n",
    "as a bias to the message *before* the MLP, not used to generate a weight matrix.\n",
    "Much lighter than NNConv.\n",
    "\n",
    "```\n",
    "msg_ij = ReLU(x_j + edge_attr_ij)   # edge features as additive bias\n",
    "x_i = MLP( (1 + eps) * x_i + Σ msg_ij )\n",
    "```\n",
    "\n",
    "Requires edge_attr to have the same dimension as node features, so we\n",
    "project 95 → hidden_dim with a linear layer.\n",
    "\n",
    "### TransformerConv\n",
    "Graph Transformer with edge features projected into the attention mechanism.\n",
    "Edge features modulate the attention scores rather than generating weights.\n",
    "\n",
    "```\n",
    "alpha_ij = softmax( (W_q x_i)^T (W_k x_j + W_e edge_attr_ij) / sqrt(d) )\n",
    "x_i = Σ alpha_ij * (W_v x_j + W_e' edge_attr_ij)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17aef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINEClassifier(nn.Module):\n",
    "    \"\"\"GINEConv-based classifier with edge features as additive bias.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim=128, num_layers=2,\n",
    "                 edge_feat_dim=95, dropout=0.1, pool=\"max\", jk_mode=\"none\"):\n",
    "        super().__init__()\n",
    "        self.dropout    = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.jk_mode    = jk_mode\n",
    "        self.pool_type  = pool\n",
    "\n",
    "        # Project edge features to match node dim at each layer\n",
    "        self.edge_projs = nn.ModuleList()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            c_in = in_dim if i == 0 else hidden_dim\n",
    "\n",
    "            # GINEConv needs edge_attr same dim as node features\n",
    "            self.edge_projs.append(nn.Linear(edge_feat_dim, c_in))\n",
    "\n",
    "            # The inner MLP for GINEConv\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(c_in, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            )\n",
    "            self.convs.append(GINEConv(mlp))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # JK\n",
    "        if jk_mode != \"none\":\n",
    "            self.jk = JumpingKnowledge(\n",
    "                mode=jk_mode, channels=hidden_dim, num_layers=num_layers)\n",
    "            jk_out = hidden_dim * num_layers if jk_mode == \"cat\" else hidden_dim\n",
    "        else:\n",
    "            self.jk = None\n",
    "            jk_out = hidden_dim\n",
    "\n",
    "        # Pooling\n",
    "        if pool == \"attention\":\n",
    "            gate_nn = nn.Sequential(\n",
    "                nn.Linear(jk_out, jk_out), nn.ReLU(), nn.Linear(jk_out, 1))\n",
    "            self.pool_fn = GlobalAttention(gate_nn)\n",
    "        else:\n",
    "            self.pool_fn = global_max_pool\n",
    "\n",
    "        # MLP head\n",
    "        self.lin1 = nn.Linear(jk_out, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "        xs = []\n",
    "        for i, (conv, bn, edge_proj) in enumerate(\n",
    "                zip(self.convs, self.bns, self.edge_projs)):\n",
    "            ea = edge_proj(edge_attr)\n",
    "            x = conv(x, edge_index, ea)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "\n",
    "        x = self.jk(xs) if self.jk is not None else xs[-1]\n",
    "        g = self.pool_fn(x, batch) if self.pool_type == \"attention\" else self.pool_fn(x, batch)\n",
    "        g = F.relu(self.lin1(g))\n",
    "        if self.dropout > 0:\n",
    "            g = F.dropout(g, p=self.dropout, training=self.training)\n",
    "        return self.lin2(g)\n",
    "\n",
    "\n",
    "class TransformerConvClassifier(nn.Module):\n",
    "    \"\"\"TransformerConv-based classifier with edge features in attention.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim=128, num_layers=2,\n",
    "                 edge_feat_dim=95, heads=2, dropout=0.1,\n",
    "                 pool=\"max\", jk_mode=\"none\"):\n",
    "        super().__init__()\n",
    "        self.dropout    = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.jk_mode    = jk_mode\n",
    "        self.pool_type  = pool\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            c_in = in_dim if i == 0 else hidden_dim\n",
    "\n",
    "            if i < num_layers - 1:\n",
    "                # multi-head with concat\n",
    "                per_head = max(hidden_dim // heads, 1)\n",
    "                self.convs.append(TransformerConv(\n",
    "                    c_in, per_head, heads=heads,\n",
    "                    edge_dim=edge_feat_dim, concat=True, dropout=dropout))\n",
    "                self.bns.append(nn.BatchNorm1d(per_head * heads))\n",
    "            else:\n",
    "                # final layer: single head\n",
    "                self.convs.append(TransformerConv(\n",
    "                    c_in, hidden_dim, heads=1,\n",
    "                    edge_dim=edge_feat_dim, concat=False, dropout=dropout))\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # JK\n",
    "        if jk_mode != \"none\":\n",
    "            self.jk = JumpingKnowledge(\n",
    "                mode=jk_mode, channels=hidden_dim, num_layers=num_layers)\n",
    "            jk_out = hidden_dim * num_layers if jk_mode == \"cat\" else hidden_dim\n",
    "        else:\n",
    "            self.jk = None\n",
    "            jk_out = hidden_dim\n",
    "\n",
    "        # Pooling\n",
    "        if pool == \"attention\":\n",
    "            gate_nn = nn.Sequential(\n",
    "                nn.Linear(jk_out, jk_out), nn.ReLU(), nn.Linear(jk_out, 1))\n",
    "            self.pool_fn = GlobalAttention(gate_nn)\n",
    "        else:\n",
    "            self.pool_fn = global_max_pool\n",
    "\n",
    "        # MLP head\n",
    "        self.lin1 = nn.Linear(jk_out, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "        xs = []\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "\n",
    "        x = self.jk(xs) if self.jk is not None else xs[-1]\n",
    "        g = self.pool_fn(x, batch) if self.pool_type == \"attention\" else self.pool_fn(x, batch)\n",
    "        g = F.relu(self.lin1(g))\n",
    "        if self.dropout > 0:\n",
    "            g = F.dropout(g, p=self.dropout, training=self.training)\n",
    "        return self.lin2(g)\n",
    "\n",
    "\n",
    "# Smoke test both architectures\n",
    "_sample = train_ds[0]\n",
    "_sample.batch = torch.zeros(_sample.x.size(0), dtype=torch.long)\n",
    "\n",
    "print(\"GINEConv:\")\n",
    "for _h in [64, 128]:\n",
    "    _m = GINEClassifier(in_dim=IN_DIM, hidden_dim=_h, num_layers=2)\n",
    "    with torch.no_grad():\n",
    "        _out = _m(_sample)\n",
    "    _np = sum(p.numel() for p in _m.parameters())\n",
    "    print(f\"  hidden={_h:3d} | params={_np:>8,} | out={tuple(_out.shape)}\")\n",
    "\n",
    "print(\"TransformerConv:\")\n",
    "for _h in [64, 128]:\n",
    "    _m = TransformerConvClassifier(in_dim=IN_DIM, hidden_dim=_h, num_layers=2, heads=2)\n",
    "    with torch.no_grad():\n",
    "        _out = _m(_sample)\n",
    "    _np = sum(p.numel() for p in _m.parameters())\n",
    "    print(f\"  hidden={_h:3d} | params={_np:>8,} | out={tuple(_out.shape)}\")\n",
    "\n",
    "print(\"\\nBoth architectures pass smoke test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90538425",
   "metadata": {},
   "source": [
    "## 4. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ss = [], []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        probs  = F.softmax(logits, dim=1)[:, 1]\n",
    "        ys.append(batch.y.view(-1).cpu().numpy())\n",
    "        ss.append(probs.cpu().numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ss)\n",
    "\n",
    "\n",
    "def optimal_f1_threshold(y_true, y_score):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
    "    f1 = (2 * prec * rec) / (prec + rec + 1e-12)\n",
    "    idx = int(np.nanargmax(f1))\n",
    "    return float(thr[max(idx - 1, 0)]) if len(thr) else 0.5\n",
    "\n",
    "\n",
    "def compute_test_metrics(y_true, y_score, threshold):\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return {\n",
    "        \"test_pr_auc\":    float(average_precision_score(y_true, y_score)),\n",
    "        \"test_roc_auc\":   float(roc_auc_score(y_true, y_score)),\n",
    "        \"test_f1\":        float(f1_score(y_true, y_pred)),\n",
    "        \"test_precision\": float(tp / max(tp + fp, 1)),\n",
    "        \"test_recall\":    float(tp / max(tp + fn, 1)),\n",
    "        \"test_threshold\": float(threshold),\n",
    "        \"test_confusion\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325564a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(*, arch, hidden_dim, num_layers, dropout, lr,\n",
    "                       pool, jk_mode, heads=2,\n",
    "                       datasets=None, edge_feat_dim=None,\n",
    "                       trial=None, verbose=False):\n",
    "    \"\"\"Train one GINEConv or TransformerConv configuration.\"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    if datasets is not None:\n",
    "        _train_ds, _val_ds, _test_ds = datasets\n",
    "    else:\n",
    "        _train_ds, _val_ds, _test_ds = train_ds, val_ds, test_ds\n",
    "\n",
    "    if edge_feat_dim is None:\n",
    "        edge_feat_dim = EDGE_FEAT_DIM\n",
    "\n",
    "    train_loader = PyGDataLoader(_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = PyGDataLoader(_val_ds,   batch_size=512, shuffle=False)\n",
    "    test_loader  = PyGDataLoader(_test_ds,  batch_size=512, shuffle=False)\n",
    "\n",
    "    if arch == \"gine\":\n",
    "        model = GINEClassifier(\n",
    "            in_dim=IN_DIM, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            edge_feat_dim=edge_feat_dim, dropout=dropout,\n",
    "            pool=pool, jk_mode=jk_mode,\n",
    "        ).to(DEVICE)\n",
    "    elif arch == \"transformer\":\n",
    "        model = TransformerConvClassifier(\n",
    "            in_dim=IN_DIM, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            edge_feat_dim=edge_feat_dim, heads=heads, dropout=dropout,\n",
    "            pool=pool, jk_mode=jk_mode,\n",
    "        ).to(DEVICE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown arch: {arch}\")\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    run = None\n",
    "    if WANDB_ENABLED:\n",
    "        run = wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=f\"wb03c3_{arch}_h{hidden_dim}\",\n",
    "            tags=[\"wb03c3\", arch, f\"pool_{pool}\", f\"jk_{jk_mode}\"],\n",
    "            config=dict(notebook=\"wb03c3\", arch=arch, hidden_dim=hidden_dim,\n",
    "                        num_layers=num_layers, dropout=dropout, lr=lr,\n",
    "                        pool=pool, jk_mode=jk_mode, heads=heads,\n",
    "                        edge_feat_dim=edge_feat_dim,\n",
    "                        n_params=n_params),\n",
    "            reinit=True)\n",
    "\n",
    "    best_val_pr, best_state, best_epoch, bad_epochs = -1.0, None, 0, 0\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                loss = criterion(model(batch), batch.y.view(-1))\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(f\"  OOM at epoch {epoch}\")\n",
    "                    continue\n",
    "                raise\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch.num_graphs\n",
    "        avg_loss = epoch_loss / len(_train_ds)\n",
    "\n",
    "        yv, sv = evaluate(model, val_loader)\n",
    "        vp = float(average_precision_score(yv, sv))\n",
    "        vr = float(roc_auc_score(yv, sv))\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"  epoch {epoch:3d}  loss={avg_loss:.4f}\"\n",
    "                  f\"  val_pr={vp:.4f}  val_roc={vr:.4f}\")\n",
    "\n",
    "        if run:\n",
    "            wandb.log({\"train/loss\": avg_loss, \"val/pr_auc\": vp,\n",
    "                        \"val/roc_auc\": vr, \"epoch\": epoch})\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(vp, epoch)\n",
    "            if trial.should_prune():\n",
    "                if run:\n",
    "                    wandb.log({\"pruned\": True}); run.finish(quiet=True)\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        if vp > best_val_pr + 1e-4:\n",
    "            best_val_pr = vp\n",
    "            best_state  = {k: v.detach().cpu().clone()\n",
    "                           for k, v in model.state_dict().items()}\n",
    "            best_epoch  = epoch\n",
    "            bad_epochs  = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    wall = time.time() - t0\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    yt, st = evaluate(model, test_loader)\n",
    "    thr    = optimal_f1_threshold(yt, st)\n",
    "    tm     = compute_test_metrics(yt, st, thr)\n",
    "\n",
    "    result = dict(arch=arch, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                  dropout=dropout, lr=lr, pool=pool, jk_mode=jk_mode,\n",
    "                  heads=heads, edge_feat_dim=edge_feat_dim,\n",
    "                  n_params=n_params, best_val_pr_auc=best_val_pr,\n",
    "                  best_epoch=best_epoch, wall_seconds=wall, **tm)\n",
    "\n",
    "    if run:\n",
    "        wandb.log(dict(best_val_pr_auc=best_val_pr, best_epoch=best_epoch,\n",
    "                        n_params=n_params, wall_seconds=wall, **tm))\n",
    "        run.finish(quiet=True)\n",
    "\n",
    "    return result, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feabfdb",
   "metadata": {},
   "source": [
    "## 5. Optuna Search — GINEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bbb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gine_objective():\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256])\n",
    "        num_layers = trial.suggest_categorical(\"num_layers\", [2, 3])\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.05, 0.30, step=0.05)\n",
    "        lr         = trial.suggest_float(\"lr\", 5e-4, 3e-3, log=True)\n",
    "        pool       = trial.suggest_categorical(\"pool\",    [\"max\", \"attention\"])\n",
    "        jk_mode    = trial.suggest_categorical(\"jk_mode\", [\"none\", \"cat\"])\n",
    "\n",
    "        result, _ = train_and_evaluate(\n",
    "            arch=\"gine\", hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            dropout=dropout, lr=lr, pool=pool, jk_mode=jk_mode, trial=trial)\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "    return objective\n",
    "\n",
    "gine_study = optuna.create_study(\n",
    "    study_name=\"wb03c3_gine\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    ")\n",
    "gine_study.optimize(make_gine_objective(), n_trials=N_TRIALS,\n",
    "                    show_progress_bar=True)\n",
    "\n",
    "gine_results = [t.user_attrs[\"result\"]\n",
    "                for t in gine_study.trials\n",
    "                if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "n_pruned = sum(1 for t in gine_study.trials\n",
    "               if t.state == optuna.trial.TrialState.PRUNED)\n",
    "print(f\"\\nGINEConv — Completed: {len(gine_results)}/{N_TRIALS}  Pruned: {n_pruned}\")\n",
    "print(f\"Best trial #{gine_study.best_trial.number}  \"\n",
    "      f\"val PR-AUC = {gine_study.best_value:.4f}\")\n",
    "for k, v in gine_study.best_trial.params.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "(WB03C3_DIR / \"gine_search_results.json\").write_text(\n",
    "    json.dumps(gine_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43799771",
   "metadata": {},
   "source": [
    "## 6. Optuna Search — TransformerConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424edcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformer_objective():\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256])\n",
    "        num_layers = trial.suggest_categorical(\"num_layers\", [2, 3])\n",
    "        heads      = trial.suggest_categorical(\"heads\", [1, 2, 4])\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.05, 0.30, step=0.05)\n",
    "        lr         = trial.suggest_float(\"lr\", 5e-4, 3e-3, log=True)\n",
    "        pool       = trial.suggest_categorical(\"pool\",    [\"max\", \"attention\"])\n",
    "        jk_mode    = trial.suggest_categorical(\"jk_mode\", [\"none\", \"cat\"])\n",
    "\n",
    "        result, _ = train_and_evaluate(\n",
    "            arch=\"transformer\", hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            heads=heads, dropout=dropout, lr=lr, pool=pool, jk_mode=jk_mode,\n",
    "            trial=trial)\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "    return objective\n",
    "\n",
    "transformer_study = optuna.create_study(\n",
    "    study_name=\"wb03c3_transformer\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED + 1),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    ")\n",
    "transformer_study.optimize(make_transformer_objective(), n_trials=N_TRIALS,\n",
    "                           show_progress_bar=True)\n",
    "\n",
    "transformer_results = [t.user_attrs[\"result\"]\n",
    "                       for t in transformer_study.trials\n",
    "                       if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "n_pruned = sum(1 for t in transformer_study.trials\n",
    "               if t.state == optuna.trial.TrialState.PRUNED)\n",
    "print(f\"\\nTransformerConv — Completed: {len(transformer_results)}/{N_TRIALS}  \"\n",
    "      f\"Pruned: {n_pruned}\")\n",
    "print(f\"Best trial #{transformer_study.best_trial.number}  \"\n",
    "      f\"val PR-AUC = {transformer_study.best_value:.4f}\")\n",
    "for k, v in transformer_study.best_trial.params.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "(WB03C3_DIR / \"transformer_search_results.json\").write_text(\n",
    "    json.dumps(transformer_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdc43c",
   "metadata": {},
   "source": [
    "## 7. Dense Feature Variants\n",
    "\n",
    "Run the best config from each architecture on the dense feature subset.\n",
    "Fewer features = the edge projection/attention has less noise to deal with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS_DENSE = 2 if SMOKE_TEST else 10\n",
    "\n",
    "# GINEConv dense\n",
    "def make_gine_dense_objective():\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256])\n",
    "        num_layers = trial.suggest_categorical(\"num_layers\", [2, 3])\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.05, 0.30, step=0.05)\n",
    "        lr         = trial.suggest_float(\"lr\", 5e-4, 3e-3, log=True)\n",
    "        pool       = trial.suggest_categorical(\"pool\",    [\"max\", \"attention\"])\n",
    "        jk_mode    = trial.suggest_categorical(\"jk_mode\", [\"none\", \"cat\"])\n",
    "\n",
    "        result, _ = train_and_evaluate(\n",
    "            arch=\"gine\", hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            dropout=dropout, lr=lr, pool=pool, jk_mode=jk_mode,\n",
    "            datasets=(train_ds_dense, val_ds_dense, test_ds_dense),\n",
    "            edge_feat_dim=DENSE_FEAT_DIM, trial=trial)\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "    return objective\n",
    "\n",
    "print(f\"GINEConv dense ({DENSE_FEAT_DIM} features), {N_TRIALS_DENSE} trials\")\n",
    "gine_dense_study = optuna.create_study(\n",
    "    study_name=\"wb03c3_gine_dense\", direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED + 2),\n",
    "    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10))\n",
    "gine_dense_study.optimize(make_gine_dense_objective(),\n",
    "                          n_trials=N_TRIALS_DENSE, show_progress_bar=True)\n",
    "\n",
    "gine_dense_results = [t.user_attrs[\"result\"]\n",
    "                      for t in gine_dense_study.trials\n",
    "                      if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "print(f\"  Best val PR-AUC: {gine_dense_study.best_value:.4f}\")\n",
    "\n",
    "# TransformerConv dense\n",
    "def make_transformer_dense_objective():\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256])\n",
    "        num_layers = trial.suggest_categorical(\"num_layers\", [2, 3])\n",
    "        heads      = trial.suggest_categorical(\"heads\", [1, 2, 4])\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.05, 0.30, step=0.05)\n",
    "        lr         = trial.suggest_float(\"lr\", 5e-4, 3e-3, log=True)\n",
    "        pool       = trial.suggest_categorical(\"pool\",    [\"max\", \"attention\"])\n",
    "        jk_mode    = trial.suggest_categorical(\"jk_mode\", [\"none\", \"cat\"])\n",
    "\n",
    "        result, _ = train_and_evaluate(\n",
    "            arch=\"transformer\", hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            heads=heads, dropout=dropout, lr=lr, pool=pool, jk_mode=jk_mode,\n",
    "            datasets=(train_ds_dense, val_ds_dense, test_ds_dense),\n",
    "            edge_feat_dim=DENSE_FEAT_DIM, trial=trial)\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "    return objective\n",
    "\n",
    "print(f\"\\nTransformerConv dense ({DENSE_FEAT_DIM} features), {N_TRIALS_DENSE} trials\")\n",
    "transformer_dense_study = optuna.create_study(\n",
    "    study_name=\"wb03c3_transformer_dense\", direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED + 3),\n",
    "    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10))\n",
    "transformer_dense_study.optimize(make_transformer_dense_objective(),\n",
    "                                 n_trials=N_TRIALS_DENSE, show_progress_bar=True)\n",
    "\n",
    "transformer_dense_results = [t.user_attrs[\"result\"]\n",
    "                             for t in transformer_dense_study.trials\n",
    "                             if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "print(f\"  Best val PR-AUC: {transformer_dense_study.best_value:.4f}\")\n",
    "\n",
    "# Save all dense results\n",
    "(WB03C3_DIR / \"gine_dense_results.json\").write_text(\n",
    "    json.dumps(gine_dense_results, indent=2))\n",
    "(WB03C3_DIR / \"transformer_dense_results.json\").write_text(\n",
    "    json.dumps(transformer_dense_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cb3ea",
   "metadata": {},
   "source": [
    "## 8. Results Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = gine_results + transformer_results\n",
    "df = pd.DataFrame(all_results).sort_values(\"test_pr_auc\", ascending=False)\n",
    "\n",
    "cols = [\"arch\", \"hidden_dim\", \"num_layers\", \"jk_mode\", \"pool\", \"heads\",\n",
    "        \"dropout\", \"lr\", \"best_val_pr_auc\", \"test_pr_auc\", \"test_roc_auc\",\n",
    "        \"test_f1\", \"test_precision\", \"test_recall\", \"n_params\",\n",
    "        \"best_epoch\", \"wall_seconds\"]\n",
    "cols = [c for c in cols if c in df.columns]\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"ALL COMPLETED TRIALS (by test PR-AUC)\")\n",
    "print(\"=\"*90)\n",
    "print(df[cols].head(20).to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Per-architecture bests\n",
    "for arch_name, study in [(\"GINEConv\", gine_study),\n",
    "                          (\"TransformerConv\", transformer_study)]:\n",
    "    best = study.best_trial.user_attrs[\"result\"]\n",
    "    print(f\"\\n{arch_name} best:\")\n",
    "    print(f\"  Test PR-AUC: {best['test_pr_auc']:.4f}  \"\n",
    "          f\"ROC-AUC: {best['test_roc_auc']:.4f}  F1: {best['test_f1']:.4f}\")\n",
    "    print(f\"  Params: {best['n_params']:,}\")\n",
    "\n",
    "# Dense comparison\n",
    "print(\"\\nDENSE FEATURE RESULTS:\")\n",
    "for name, study in [(\"GINEConv dense\", gine_dense_study),\n",
    "                     (\"TransformerConv dense\", transformer_dense_study)]:\n",
    "    best = study.best_trial.user_attrs[\"result\"]\n",
    "    print(f\"  {name}: test PR-AUC={best['test_pr_auc']:.4f}  \"\n",
    "          f\"params={best['n_params']:,}\")\n",
    "\n",
    "# Full comparison\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FULL BASELINE COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "gine_best = gine_study.best_trial.user_attrs[\"result\"]\n",
    "trans_best = transformer_study.best_trial.user_attrs[\"result\"]\n",
    "gine_d_best = gine_dense_study.best_trial.user_attrs[\"result\"]\n",
    "trans_d_best = transformer_dense_study.best_trial.user_attrs[\"result\"]\n",
    "\n",
    "for name, v in [\n",
    "    (\"Random (prevalence)\",        0.023),\n",
    "    (\"LogReg pooled (Wb02)\",       0.154),\n",
    "    (\"GLASS (Bellei 2024)\",        0.208),\n",
    "    (\"SAGE tuned (Wb03)\",          0.4848),\n",
    "    (\"GATv2 tuned (Wb03)\",         0.4964),\n",
    "    (\"GINEConv (all 95)\",          gine_best[\"test_pr_auc\"]),\n",
    "    (f\"GINEConv (dense {DENSE_FEAT_DIM})\", gine_d_best[\"test_pr_auc\"]),\n",
    "    (\"TransformerConv (all 95)\",   trans_best[\"test_pr_auc\"]),\n",
    "    (f\"TransformerConv (dense {DENSE_FEAT_DIM})\", trans_d_best[\"test_pr_auc\"]),\n",
    "]:\n",
    "    print(f\"  {name:35s} {v:.4f}  {'█' * int(v * 100)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49950655",
   "metadata": {},
   "source": [
    "## 9. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0455397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the overall best across both architectures\n",
    "best_row = df.iloc[0].to_dict()\n",
    "best_arch = best_row[\"arch\"]\n",
    "print(f\"Overall best: {best_arch}\")\n",
    "print(f\"Re-training for checkpoint...\")\n",
    "\n",
    "res_best, state_best = train_and_evaluate(\n",
    "    arch=best_row[\"arch\"],\n",
    "    hidden_dim=int(best_row[\"hidden_dim\"]),\n",
    "    num_layers=int(best_row[\"num_layers\"]),\n",
    "    dropout=float(best_row[\"dropout\"]),\n",
    "    lr=float(best_row[\"lr\"]),\n",
    "    pool=best_row[\"pool\"],\n",
    "    jk_mode=best_row[\"jk_mode\"],\n",
    "    heads=int(best_row.get(\"heads\", 2)),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": state_best,\n",
    "    \"config\": {k: best_row[k] for k in\n",
    "               [\"arch\",\"hidden_dim\",\"num_layers\",\"dropout\",\"lr\",\n",
    "                \"pool\",\"jk_mode\",\"heads\"] if k in best_row},\n",
    "    \"metrics\": {k: res_best[k] for k in\n",
    "                [\"best_val_pr_auc\",\"test_pr_auc\",\"test_roc_auc\",\n",
    "                 \"test_f1\",\"test_precision\",\"test_recall\"]},\n",
    "    \"in_dim\": IN_DIM,\n",
    "    \"edge_feat_dim\": EDGE_FEAT_DIM,\n",
    "}, WB03C3_DIR / \"best_model.pt\")\n",
    "print(f\"Saved → {WB03C3_DIR / 'best_model.pt'}  \"\n",
    "      f\"PR-AUC={res_best['test_pr_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5b2b3",
   "metadata": {},
   "source": [
    "## 10. Optuna Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e649e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for col, (name, study) in enumerate([(\"GINEConv\", gine_study),\n",
    "                                      (\"TransformerConv\", transformer_study)]):\n",
    "    vals = [t.value for t in study.trials if t.value is not None]\n",
    "    axes[0, col].plot(vals, \"o-\", ms=4, alpha=0.7)\n",
    "    axes[0, col].axhline(study.best_value, color=\"red\", ls=\"--\", alpha=0.5,\n",
    "                          label=f\"Best: {study.best_value:.4f}\")\n",
    "    axes[0, col].axhline(0.4964, color=\"blue\", ls=\":\", alpha=0.5,\n",
    "                          label=\"GATv2 Wb03: 0.4964\")\n",
    "    axes[0, col].set(xlabel=\"Trial\", ylabel=\"Val PR-AUC\",\n",
    "                      title=f\"{name} — Optimisation History\")\n",
    "    axes[0, col].legend(fontsize=8)\n",
    "\n",
    "    try:\n",
    "        imp = optuna.importance.get_param_importances(study)\n",
    "        ps = list(imp.keys())[:6]\n",
    "        axes[1, col].barh(ps[::-1], [imp[p] for p in ps[::-1]])\n",
    "        axes[1, col].set(xlabel=\"Importance\",\n",
    "                          title=f\"{name} — Parameter Importance\")\n",
    "    except Exception as e:\n",
    "        axes[1, col].text(0.5, 0.5, str(e), ha=\"center\", va=\"center\",\n",
    "                           transform=axes[1, col].transAxes)\n",
    "\n",
    "save_fig(WB03C3_DIR / \"optuna_diagnostics.png\")\n",
    "plt.show()\n",
    "print(f\"Saved: {WB03C3_DIR / 'optuna_diagnostics.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d216c",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "**Outputs saved to `results/wb03c3/`:**\n",
    "- `gine_search_results.json` / `transformer_search_results.json`\n",
    "- `gine_dense_results.json` / `transformer_dense_results.json`\n",
    "- `best_model.pt`\n",
    "- `optuna_diagnostics.png`\n",
    "\n",
    "**Key questions answered:**\n",
    "1. Do lighter edge feature architectures outperform NNConv on tiny subgraphs?\n",
    "2. Does TransformerConv's edge-aware attention beat GATv2 (no edge features)?\n",
    "3. Does GINEConv's additive edge bias help despite its simplicity?\n",
    "4. Do dense features perform comparably to all 95?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
