{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1503d2ba",
   "metadata": {},
   "source": [
    "# Workbook 03c2 — NNConv with Edge Features\n",
    "\n",
    "**Objective:** Test whether the 95 edge features from `background_edges.csv`\n",
    "improve subgraph classification via NNConv edge-conditioned message passing.\n",
    "\n",
    "**Architecture:** NNConv replaces GNN backbone. Each edge has an MLP\n",
    "that maps its 95-dim feature vector to a weight matrix for message passing:\n",
    "\n",
    "```\n",
    "edge_nn: R^95 → R^(in_dim × out_dim)\n",
    "conv = NNConv(in_dim, out_dim, edge_nn, aggr='add')\n",
    "```\n",
    "\n",
    "**Early stopping rule:** If after 15 completed trials, best val PR-AUC < 0.50,\n",
    "stop (edge features not helping enough — still a publishable negative result).\n",
    "\n",
    "**Budget:** 25–35 trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b2237",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force non-interactive backend for script/tmux runs\n",
    "os.environ.setdefault(\"MPLBACKEND\", \"Agg\")\n",
    "import json, random, time, warnings\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_fig(path, **kwargs):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\", **kwargs)\n",
    "    plt.close()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    f1_score, precision_recall_curve, confusion_matrix,\n",
    ")\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import NNConv, global_max_pool, GlobalAttention, JumpingKnowledge\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "RNG_SEED = 7\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "PROJECT_ROOT  = Path.cwd()\n",
    "DATA_DIR      = PROJECT_ROOT / \"DATA\"\n",
    "PROCESSED     = DATA_DIR / \"processed\"\n",
    "ARRAYS_DIR    = PROCESSED / \"arrays\"\n",
    "ARTIFACTS_DIR = PROCESSED / \"artifacts\"\n",
    "PACK_DIR      = ARTIFACTS_DIR / \"packed\"\n",
    "RESULTS_DIR   = PROJECT_ROOT / \"results\"\n",
    "WB03C2_DIR    = RESULTS_DIR / \"wb03c2\"\n",
    "WB03C2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"  GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "SMOKE_TEST    = False\n",
    "N_TRIALS      = 2  if SMOKE_TEST else 30\n",
    "MAX_EPOCHS    = 5  if SMOKE_TEST else 80\n",
    "PATIENCE      = 3  if SMOKE_TEST else 15\n",
    "BATCH_SIZE    = 256\n",
    "WANDB_PROJECT = \"elliptic2-gnn\"\n",
    "WANDB_ENABLED = not SMOKE_TEST\n",
    "\n",
    "# Early stopping rule for the search itself\n",
    "EARLY_STOP_CHECK_TRIAL = 15\n",
    "EARLY_STOP_THRESHOLD   = 0.50\n",
    "\n",
    "print(f\"SMOKE_TEST={SMOKE_TEST}  trials={N_TRIALS}  epochs={MAX_EPOCHS}\")\n",
    "print(f\"Early stop: check at trial {EARLY_STOP_CHECK_TRIAL}, \"\n",
    "      f\"threshold={EARLY_STOP_THRESHOLD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import display, clear_output\n",
    "except Exception:\n",
    "    display = None\n",
    "    clear_output = None\n",
    "\n",
    "def safe_display(obj):\n",
    "    if display:\n",
    "        safe_display(obj)\n",
    "    else:\n",
    "        print(obj)\n",
    "\n",
    "def safe_clear_output():\n",
    "    if clear_output:\n",
    "        clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404a3db",
   "metadata": {},
   "source": [
    "## 1. Load Data + Edge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41091a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Core arrays ──────────────────────────────────────────────\n",
    "X = np.load(ARRAYS_DIR / \"node_features.npy\")\n",
    "subgraph_labels = {\n",
    "    int(k): v for k, v in\n",
    "    json.loads((ARTIFACTS_DIR / \"subgraph_labels.json\").read_text()).items()\n",
    "}\n",
    "splits = json.loads((ARTIFACTS_DIR / \"splits.json\").read_text())\n",
    "\n",
    "# ── Packed arrays ────────────────────────────────────────────\n",
    "nodes_pack        = np.load(PACK_DIR / \"nodes_by_ccid.npz\")\n",
    "edges_pack        = np.load(PACK_DIR / \"edges_by_ccid.npz\")\n",
    "unique_cc         = nodes_pack[\"unique_cc\"].astype(np.int64)\n",
    "node_ptr          = nodes_pack[\"node_ptr\"].astype(np.int64)\n",
    "node_row_perm     = nodes_pack[\"node_row_perm\"].astype(np.int64)\n",
    "unique_cc_edges   = edges_pack[\"unique_cc_edges\"].astype(np.int64)\n",
    "edge_ptr          = edges_pack[\"edge_ptr\"].astype(np.int64)\n",
    "edge_src_row_perm = edges_pack[\"edge_src_row_perm\"].astype(np.int64)\n",
    "edge_dst_row_perm = edges_pack[\"edge_dst_row_perm\"].astype(np.int64)\n",
    "\n",
    "ccid_to_i  = {int(c): i for i, c in enumerate(unique_cc)}\n",
    "ccid_to_ei = {int(c): i for i, c in enumerate(unique_cc_edges)}\n",
    "\n",
    "def label_to_int(lbl):\n",
    "    return 1 if str(lbl).lower() in {\"suspicious\", \"illicit\"} else 0\n",
    "\n",
    "y_by_cc = {int(c): label_to_int(subgraph_labels[int(c)]) for c in unique_cc}\n",
    "\n",
    "# ── Edge features (from Wb03c1) ──────────────────────────────\n",
    "EDGE_FEATURES = np.load(ARRAYS_DIR / \"edge_features.npy\")  # (n_edges, 95)\n",
    "EDGE_FEAT_DIM = EDGE_FEATURES.shape[1]\n",
    "print(f\"Edge features loaded: {EDGE_FEATURES.shape}\")\n",
    "\n",
    "IN_DIM = X.shape[1]  # 43\n",
    "print(f\"Node features: {X.shape}  Edge features: {EDGE_FEATURES.shape}\")\n",
    "print(f\"Subgraphs: {len(unique_cc):,}\")\n",
    "\n",
    "# Dense feature subset (from Wb03c1)\n",
    "EDGE_FEATURES_DENSE = np.load(ARRAYS_DIR / \"edge_features_dense.npy\")\n",
    "DENSE_FEAT_DIM = EDGE_FEATURES_DENSE.shape[1]\n",
    "dense_meta = json.loads((ARTIFACTS_DIR / \"edge_feature_dense_meta.json\").read_text())\n",
    "\n",
    "print(f\"Dense features loaded: {EDGE_FEATURES_DENSE.shape} \"\n",
    "      f\"(zero rate < {dense_meta['zero_threshold']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c086f8b",
   "metadata": {},
   "source": [
    "## 2. Dataset with Edge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elliptic2EdgeFeatureDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyG dataset with edge features for NNConv.\n",
    "\n",
    "    Returns Data objects with:\n",
    "        x:          [n_nodes, 43]\n",
    "        edge_index: [2, n_edges] (undirected)\n",
    "        edge_attr:  [n_edges, 95] (edge features, duplicated for reverse edges)\n",
    "        y:          [1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ccids, edge_features=None, use_random_edges=False):\n",
    "        self.ccids = np.asarray(ccids, dtype=np.int64)\n",
    "        self.edge_features = edge_features if edge_features is not None else EDGE_FEATURES\n",
    "        self.use_random_edges = use_random_edges\n",
    "        self._rng = np.random.RandomState(RNG_SEED)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ccids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ccid = int(self.ccids[idx])\n",
    "        i    = ccid_to_i[ccid]\n",
    "        rows = node_row_perm[node_ptr[i] : node_ptr[i + 1]]\n",
    "        x    = torch.from_numpy(X[rows]).float()\n",
    "\n",
    "        local = {int(r): j for j, r in enumerate(rows.tolist())}\n",
    "\n",
    "        if ccid in ccid_to_ei:\n",
    "            ei = ccid_to_ei[ccid]\n",
    "            s  = edge_src_row_perm[edge_ptr[ei] : edge_ptr[ei + 1]]\n",
    "            t  = edge_dst_row_perm[edge_ptr[ei] : edge_ptr[ei + 1]]\n",
    "\n",
    "            # Global edge indices for feature lookup\n",
    "            global_edge_start = edge_ptr[ei]\n",
    "            global_edge_end   = edge_ptr[ei + 1]\n",
    "\n",
    "            src = torch.tensor([local[int(r)] for r in s], dtype=torch.long)\n",
    "            dst = torch.tensor([local[int(r)] for r in t], dtype=torch.long)\n",
    "\n",
    "            # Original directed edges\n",
    "            edge_index_dir = torch.stack([src, dst], dim=0)\n",
    "\n",
    "            # Edge features for directed edges\n",
    "            e_feats = self.edge_features[global_edge_start:global_edge_end]\n",
    "            if self.use_random_edges:\n",
    "                e_feats = self._rng.randn(*e_feats.shape).astype(np.float32)\n",
    "            edge_attr_dir = torch.from_numpy(e_feats.copy()).float()\n",
    "\n",
    "            # Make undirected: add reverse edges with same features\n",
    "            edge_index_rev = torch.stack([dst, src], dim=0)\n",
    "            edge_index = torch.cat([edge_index_dir, edge_index_rev], dim=1)\n",
    "            edge_attr  = torch.cat([edge_attr_dir, edge_attr_dir], dim=0)\n",
    "\n",
    "            # Remove duplicate edges (to_undirected equivalent)\n",
    "            # Use coalesce to handle duplicates\n",
    "            from torch_geometric.utils import coalesce\n",
    "            edge_index, edge_attr = coalesce(\n",
    "                edge_index, edge_attr, reduce=\"mean\"\n",
    "            )\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr  = torch.empty((0, EDGE_FEAT_DIM), dtype=torch.float32)\n",
    "\n",
    "        y = torch.tensor([y_by_cc[ccid]], dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                    y=y, ccId=ccid)\n",
    "\n",
    "\n",
    "# ── Splits ───────────────────────────────────────────────────\n",
    "train_cc = np.array(splits[\"train\"], dtype=np.int64)\n",
    "val_cc   = np.array(splits[\"val\"],   dtype=np.int64)\n",
    "test_cc  = np.array(splits[\"test\"],  dtype=np.int64)\n",
    "\n",
    "train_ds = Elliptic2EdgeFeatureDataset(train_cc)\n",
    "val_ds   = Elliptic2EdgeFeatureDataset(val_cc)\n",
    "test_ds  = Elliptic2EdgeFeatureDataset(test_cc)\n",
    "\n",
    "# Random-edge ablation datasets\n",
    "train_ds_rand = Elliptic2EdgeFeatureDataset(train_cc, use_random_edges=True)\n",
    "val_ds_rand   = Elliptic2EdgeFeatureDataset(val_cc, use_random_edges=True)\n",
    "test_ds_rand  = Elliptic2EdgeFeatureDataset(test_cc, use_random_edges=True)\n",
    "\n",
    "# Dense-feature-only datasets (features with zero rate < 0.5)\n",
    "train_ds_dense = Elliptic2EdgeFeatureDataset(train_cc, edge_features=EDGE_FEATURES_DENSE)\n",
    "val_ds_dense   = Elliptic2EdgeFeatureDataset(val_cc,   edge_features=EDGE_FEATURES_DENSE)\n",
    "test_ds_dense  = Elliptic2EdgeFeatureDataset(test_cc,  edge_features=EDGE_FEATURES_DENSE)\n",
    "\n",
    "# ── Class weights ────────────────────────────────────────────\n",
    "train_labels = torch.tensor([y_by_cc[int(c)] for c in train_cc])\n",
    "n_pos = int(train_labels.sum().item())\n",
    "n_neg = int((train_labels == 0).sum().item())\n",
    "CLASS_WEIGHTS = torch.tensor(\n",
    "    [1.0, n_neg / max(n_pos, 1)], dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Class weights: {CLASS_WEIGHTS.tolist()}\")\n",
    "print(f\"Sample with edge features: {train_ds[0]}\")\n",
    "print(f\"  edge_attr shape: {train_ds[0].edge_attr.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f41220",
   "metadata": {},
   "source": [
    "## 3. NNConv Model\n",
    "\n",
    "The edge network maps each edge's 95-dim feature vector to a weight matrix\n",
    "that conditions the message passing:\n",
    "\n",
    "```\n",
    "f_θ : R^95 → R^(in_dim × out_dim)\n",
    "msg_ij = f_θ(edge_attr_ij) · x_j\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNConvClassifier(nn.Module):\n",
    "    \"\"\"NNConv-based subgraph classifier with edge-conditioned message passing.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim=128, num_layers=2,\n",
    "                 edge_feat_dim=95, edge_hidden=64, dropout=0.1,\n",
    "                 pool=\"max\", jk_mode=\"none\", aggr=\"add\"):\n",
    "        super().__init__()\n",
    "        self.dropout    = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.jk_mode    = jk_mode\n",
    "        self.pool_type  = pool\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            c_in  = in_dim if i == 0 else hidden_dim\n",
    "            c_out = hidden_dim\n",
    "\n",
    "            # Edge network: 95 → edge_hidden → (c_in * c_out)\n",
    "            edge_nn = nn.Sequential(\n",
    "                nn.Linear(edge_feat_dim, edge_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(edge_hidden, c_in * c_out),\n",
    "            )\n",
    "            self.convs.append(NNConv(c_in, c_out, edge_nn, aggr=aggr))\n",
    "            self.bns.append(nn.BatchNorm1d(c_out))\n",
    "\n",
    "        # JumpingKnowledge\n",
    "        if jk_mode != \"none\":\n",
    "            self.jk = JumpingKnowledge(\n",
    "                mode=jk_mode, channels=hidden_dim, num_layers=num_layers)\n",
    "            jk_out = hidden_dim * num_layers if jk_mode == \"cat\" else hidden_dim\n",
    "        else:\n",
    "            self.jk = None\n",
    "            jk_out = hidden_dim\n",
    "\n",
    "        # Pooling\n",
    "        if pool == \"attention\":\n",
    "            gate_nn = nn.Sequential(\n",
    "                nn.Linear(jk_out, jk_out), nn.ReLU(), nn.Linear(jk_out, 1))\n",
    "            self.pool_fn = GlobalAttention(gate_nn)\n",
    "        else:\n",
    "            self.pool_fn = global_max_pool\n",
    "\n",
    "        # MLP head\n",
    "        self.lin1 = nn.Linear(jk_out, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "        xs = []\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "\n",
    "        x = self.jk(xs) if self.jk is not None else xs[-1]\n",
    "\n",
    "        if self.pool_type == \"attention\":\n",
    "            g = self.pool_fn(x, batch)\n",
    "        else:\n",
    "            g = self.pool_fn(x, batch)\n",
    "\n",
    "        g = F.relu(self.lin1(g))\n",
    "        if self.dropout > 0:\n",
    "            g = F.dropout(g, p=self.dropout, training=self.training)\n",
    "        return self.lin2(g)\n",
    "\n",
    "\n",
    "# ── Smoke test ───────────────────────────────────────────────\n",
    "_sample = train_ds[0]\n",
    "_sample.batch = torch.zeros(_sample.x.size(0), dtype=torch.long)\n",
    "for _h in [64, 128]:\n",
    "    for _pool in [\"max\", \"attention\"]:\n",
    "        _m = NNConvClassifier(\n",
    "            in_dim=IN_DIM, hidden_dim=_h, num_layers=2,\n",
    "            edge_hidden=64, pool=_pool, jk_mode=\"none\")\n",
    "        with torch.no_grad():\n",
    "            _out = _m(_sample)\n",
    "        _np = sum(p.numel() for p in _m.parameters())\n",
    "        print(f\"hidden={_h:3d} pool={_pool:9s} | \"\n",
    "              f\"params={_np:>9,} | out={tuple(_out.shape)}\")\n",
    "\n",
    "print(\"NNConv model passes smoke test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570fdca",
   "metadata": {},
   "source": [
    "## 4. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ss = [], []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        probs  = F.softmax(logits, dim=1)[:, 1]\n",
    "        ys.append(batch.y.view(-1).cpu().numpy())\n",
    "        ss.append(probs.cpu().numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ss)\n",
    "\n",
    "\n",
    "def optimal_f1_threshold(y_true, y_score):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
    "    f1 = (2 * prec * rec) / (prec + rec + 1e-12)\n",
    "    idx = int(np.nanargmax(f1))\n",
    "    return float(thr[max(idx - 1, 0)]) if len(thr) else 0.5\n",
    "\n",
    "\n",
    "def compute_test_metrics(y_true, y_score, threshold):\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return {\n",
    "        \"test_pr_auc\":    float(average_precision_score(y_true, y_score)),\n",
    "        \"test_roc_auc\":   float(roc_auc_score(y_true, y_score)),\n",
    "        \"test_f1\":        float(f1_score(y_true, y_pred)),\n",
    "        \"test_precision\": float(tp / max(tp + fp, 1)),\n",
    "        \"test_recall\":    float(tp / max(tp + fn, 1)),\n",
    "        \"test_threshold\": float(threshold),\n",
    "        \"test_confusion\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_nnconv(*, hidden_dim, num_layers, edge_hidden,\n",
    "                               dropout, lr, pool, jk_mode, aggr,\n",
    "                               use_random_edges=False,\n",
    "                               datasets=None, edge_feat_dim=None,\n",
    "                               trial=None, verbose=False):\n",
    "    \"\"\"Train one NNConv configuration.\"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    if datasets is not None:\n",
    "        _train_ds, _val_ds, _test_ds = datasets\n",
    "    elif use_random_edges:\n",
    "        _train_ds, _val_ds, _test_ds = train_ds_rand, val_ds_rand, test_ds_rand\n",
    "    else:\n",
    "        _train_ds, _val_ds, _test_ds = train_ds, val_ds, test_ds\n",
    "\n",
    "    if edge_feat_dim is None:\n",
    "        edge_feat_dim = EDGE_FEAT_DIM\n",
    "\n",
    "    train_loader = PyGDataLoader(_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = PyGDataLoader(_val_ds,   batch_size=512, shuffle=False)\n",
    "    test_loader  = PyGDataLoader(_test_ds,  batch_size=512, shuffle=False)\n",
    "\n",
    "    model = NNConvClassifier(\n",
    "        in_dim=IN_DIM, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "        edge_feat_dim=edge_feat_dim, edge_hidden=edge_hidden,\n",
    "        dropout=dropout, pool=pool, jk_mode=jk_mode, aggr=aggr,\n",
    "    ).to(DEVICE)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Count edge network params specifically\n",
    "    n_edge_params = sum(\n",
    "        sum(p.numel() for p in conv.nn.parameters())\n",
    "        for conv in model.convs\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # W&B\n",
    "    run = None\n",
    "    tag_list = [\"wb03c2\", \"nnconv\", f\"pool_{pool}\", f\"jk_{jk_mode}\"]\n",
    "    if use_random_edges:\n",
    "        tag_list += [\"ablation\", \"random_edges\"]\n",
    "    if WANDB_ENABLED:\n",
    "        run = wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=f\"wb03c2_nnconv_h{hidden_dim}_eh{edge_hidden}\",\n",
    "            tags=tag_list,\n",
    "            config=dict(notebook=\"wb03c2\", hidden_dim=hidden_dim,\n",
    "                        num_layers=num_layers, edge_hidden=edge_hidden,\n",
    "                        dropout=dropout, lr=lr, pool=pool, jk_mode=jk_mode,\n",
    "                        aggr=aggr, n_params=n_params, n_edge_params=n_edge_params,\n",
    "                        use_random_edges=use_random_edges),\n",
    "            reinit=True)\n",
    "\n",
    "    best_val_pr, best_state, best_epoch, bad_epochs = -1.0, None, 0, 0\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                loss = criterion(model(batch), batch.y.view(-1))\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(f\"  OOM at epoch {epoch} — skipping batch\")\n",
    "                    continue\n",
    "                raise\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch.num_graphs\n",
    "        avg_loss = epoch_loss / len(_train_ds)\n",
    "\n",
    "        yv, sv = evaluate(model, val_loader)\n",
    "        vp = float(average_precision_score(yv, sv))\n",
    "        vr = float(roc_auc_score(yv, sv))\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"  epoch {epoch:3d}  loss={avg_loss:.4f}\"\n",
    "                  f\"  val_pr={vp:.4f}  val_roc={vr:.4f}\")\n",
    "\n",
    "        if run:\n",
    "            wandb.log({\"train/loss\": avg_loss, \"val/pr_auc\": vp,\n",
    "                        \"val/roc_auc\": vr, \"epoch\": epoch})\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(vp, epoch)\n",
    "            if trial.should_prune():\n",
    "                if run:\n",
    "                    wandb.log({\"pruned\": True}); run.finish(quiet=True)\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        if vp > best_val_pr + 1e-4:\n",
    "            best_val_pr = vp\n",
    "            best_state  = {k: v.detach().cpu().clone()\n",
    "                           for k, v in model.state_dict().items()}\n",
    "            best_epoch  = epoch\n",
    "            bad_epochs  = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    wall = time.time() - t0\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    yt, st = evaluate(model, test_loader)\n",
    "    thr    = optimal_f1_threshold(yt, st)\n",
    "    tm     = compute_test_metrics(yt, st, thr)\n",
    "\n",
    "    result = dict(hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                  edge_hidden=edge_hidden, dropout=dropout, lr=lr,\n",
    "                  pool=pool, jk_mode=jk_mode, aggr=aggr,\n",
    "                  n_params=n_params, n_edge_params=n_edge_params,\n",
    "                  use_random_edges=use_random_edges,\n",
    "                  best_val_pr_auc=best_val_pr,\n",
    "                  best_epoch=best_epoch, wall_seconds=wall, **tm)\n",
    "\n",
    "    if run:\n",
    "        wandb.log(dict(best_val_pr_auc=best_val_pr, best_epoch=best_epoch,\n",
    "                        n_params=n_params, wall_seconds=wall, **tm))\n",
    "        run.finish(quiet=True)\n",
    "\n",
    "    return result, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0faf4c",
   "metadata": {},
   "source": [
    "## 5. Optuna Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopCallback:\n",
    "    \"\"\"Stop search if best val PR-AUC < threshold after N completed trials.\"\"\"\n",
    "\n",
    "    def __init__(self, check_trial, threshold):\n",
    "        self.check_trial = check_trial\n",
    "        self.threshold   = threshold\n",
    "        self._completed  = 0\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            self._completed += 1\n",
    "        if self._completed >= self.check_trial:\n",
    "            if study.best_value < self.threshold:\n",
    "                print(f\"\\n⚠ Early stop: best val PR-AUC {study.best_value:.4f}\"\n",
    "                      f\" < {self.threshold} after {self._completed} trials\")\n",
    "                study.stop()\n",
    "\n",
    "\n",
    "def make_objective():\n",
    "    def objective(trial):\n",
    "        hidden_dim  = trial.suggest_categorical(\"hidden_dim\",  [64, 128, 256])\n",
    "        num_layers  = trial.suggest_categorical(\"num_layers\",  [1, 2])\n",
    "        edge_hidden = trial.suggest_categorical(\"edge_hidden\", [64, 128])\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0.05, 0.35, step=0.05)\n",
    "        lr          = trial.suggest_float(\"lr\", 3e-4, 3e-3, log=True)\n",
    "        pool        = trial.suggest_categorical(\"pool\",    [\"max\", \"attention\"])\n",
    "        jk_mode     = trial.suggest_categorical(\"jk_mode\", [\"none\", \"cat\"])\n",
    "        aggr        = trial.suggest_categorical(\"aggr\",    [\"add\", \"mean\"])\n",
    "\n",
    "        result, _ = train_and_evaluate_nnconv(\n",
    "            hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            edge_hidden=edge_hidden, dropout=dropout, lr=lr,\n",
    "            pool=pool, jk_mode=jk_mode, aggr=aggr, trial=trial)\n",
    "\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "print(\"NNConv search space:\")\n",
    "print(\"  hidden_dim:  [64, 128, 256]\")\n",
    "print(\"  num_layers:  [1, 2]\")\n",
    "print(\"  edge_hidden: [64, 128]\")\n",
    "print(\"  dropout:     [0.05, 0.35]\")\n",
    "print(\"  lr:          [3e-4, 3e-3]\")\n",
    "print(\"  pool:        [max, attention]\")\n",
    "print(\"  jk_mode:     [none, cat]\")\n",
    "print(\"  aggr:        [add, mean]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af11108",
   "metadata": {},
   "source": [
    "### Execute Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1332f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"wb03c2_nnconv\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    ")\n",
    "\n",
    "early_stop_cb = EarlyStopCallback(\n",
    "    check_trial=EARLY_STOP_CHECK_TRIAL,\n",
    "    threshold=EARLY_STOP_THRESHOLD,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    make_objective(),\n",
    "    n_trials=N_TRIALS,\n",
    "    callbacks=[early_stop_cb],\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "all_results = [t.user_attrs[\"result\"]\n",
    "               for t in study.trials\n",
    "               if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "n_pruned = sum(1 for t in study.trials\n",
    "               if t.state == optuna.trial.TrialState.PRUNED)\n",
    "print(f\"\\nCompleted: {len(all_results)}/{len(study.trials)}  Pruned: {n_pruned}\")\n",
    "print(f\"Best trial #{study.best_trial.number}  val PR-AUC = {study.best_value:.4f}\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "(WB03C2_DIR / \"search_results.json\").write_text(json.dumps(all_results, indent=2))\n",
    "print(f\"Saved {len(all_results)} results → {WB03C2_DIR / 'search_results.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38185a36",
   "metadata": {},
   "source": [
    "## 6. Fair Comparison Ablations\n",
    "\n",
    "1. **NNConv + real edges** — best config from search (already done)\n",
    "2. **NNConv + random edges** — same architecture, random edge features\n",
    "3. **NNConv + simple config** — pool=max, jk=none (floor performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best params from search\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ABLATION 1: NNConv + random edge features\")\n",
    "print(\"=\"*60)\n",
    "res_rand, _ = train_and_evaluate_nnconv(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    edge_hidden=best_params[\"edge_hidden\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    lr=best_params[\"lr\"],\n",
    "    pool=best_params[\"pool\"],\n",
    "    jk_mode=best_params[\"jk_mode\"],\n",
    "    aggr=best_params[\"aggr\"],\n",
    "    use_random_edges=True,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"  Val PR-AUC:  {res_rand['best_val_pr_auc']:.4f}\")\n",
    "print(f\"  Test PR-AUC: {res_rand['test_pr_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ABLATION 2: NNConv simplest config (floor)\")\n",
    "print(\"=\"*60)\n",
    "res_simple, _ = train_and_evaluate_nnconv(\n",
    "    hidden_dim=128, num_layers=1, edge_hidden=64,\n",
    "    dropout=0.1, lr=1e-3, pool=\"max\", jk_mode=\"none\", aggr=\"add\",\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"  Val PR-AUC:  {res_simple['best_val_pr_auc']:.4f}\")\n",
    "print(f\"  Test PR-AUC: {res_simple['test_pr_auc']:.4f}\")\n",
    "\n",
    "ablation_results = [\n",
    "    {**study.best_trial.user_attrs[\"result\"], \"ablation\": \"best_real_edges\"},\n",
    "    {**res_rand, \"ablation\": \"random_edges\"},\n",
    "    {**res_simple, \"ablation\": \"simple_config\"},\n",
    "]\n",
    "(WB03C2_DIR / \"ablation_results.json\").write_text(\n",
    "    json.dumps(ablation_results, indent=2))\n",
    "print(\"\\nSaved ablations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c3982",
   "metadata": {},
   "source": [
    "## 6b. Dense Feature Search (reduced edge features)\n",
    "\n",
    "The c1 diagnostics showed ~half the edge features are near-empty (zero rate > 50%).\n",
    "Here we run a separate 15-trial search using only the dense features. With fewer\n",
    "input dimensions the edge MLP can be smaller, so we include edge_hidden=32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS_DENSE = 2 if SMOKE_TEST else 15\n",
    "\n",
    "def make_dense_objective():\n",
    "    def objective(trial):\n",
    "        hidden_dim  = trial.suggest_categorical(\"hidden_dim\",  [64, 128])\n",
    "        num_layers  = trial.suggest_categorical(\"num_layers\",  [1, 2])\n",
    "        edge_hidden = trial.suggest_categorical(\"edge_hidden\", [32, 64])\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0.05, 0.30, step=0.05)\n",
    "        lr          = trial.suggest_float(\"lr\", 3e-4, 3e-3, log=True)\n",
    "        pool        = trial.suggest_categorical(\"pool\",    [\"max\", \"attention\"])\n",
    "        jk_mode     = trial.suggest_categorical(\"jk_mode\", [\"none\", \"cat\"])\n",
    "        aggr        = trial.suggest_categorical(\"aggr\",    [\"add\", \"mean\"])\n",
    "\n",
    "        result, _ = train_and_evaluate_nnconv(\n",
    "            hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "            edge_hidden=edge_hidden, dropout=dropout, lr=lr,\n",
    "            pool=pool, jk_mode=jk_mode, aggr=aggr,\n",
    "            datasets=(train_ds_dense, val_ds_dense, test_ds_dense),\n",
    "            edge_feat_dim=DENSE_FEAT_DIM,\n",
    "            trial=trial)\n",
    "\n",
    "        trial.set_user_attr(\"result\", result)\n",
    "        return result[\"best_val_pr_auc\"]\n",
    "    return objective\n",
    "\n",
    "print(f\"Dense feature search: {DENSE_FEAT_DIM} features, {N_TRIALS_DENSE} trials\")\n",
    "print(f\"  edge_hidden includes 32 (smaller MLP for fewer inputs)\")\n",
    "\n",
    "dense_study = optuna.create_study(\n",
    "    study_name=\"wb03c2_nnconv_dense\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=RNG_SEED + 1),\n",
    "    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10),\n",
    ")\n",
    "dense_study.optimize(make_dense_objective(), n_trials=N_TRIALS_DENSE,\n",
    "                     show_progress_bar=True)\n",
    "\n",
    "dense_results = [t.user_attrs[\"result\"]\n",
    "                 for t in dense_study.trials\n",
    "                 if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "n_pruned_d = sum(1 for t in dense_study.trials\n",
    "                 if t.state == optuna.trial.TrialState.PRUNED)\n",
    "print(f\"\\nCompleted: {len(dense_results)}/{N_TRIALS_DENSE}  Pruned: {n_pruned_d}\")\n",
    "print(f\"Best trial #{dense_study.best_trial.number}  \"\n",
    "      f\"val PR-AUC = {dense_study.best_value:.4f}\")\n",
    "for k, v in dense_study.best_trial.params.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "(WB03C2_DIR / \"dense_search_results.json\").write_text(\n",
    "    json.dumps(dense_results, indent=2))\n",
    "print(f\"Saved {len(dense_results)} dense results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3a105",
   "metadata": {},
   "source": [
    "## 7. Results Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results).sort_values(\"test_pr_auc\", ascending=False)\n",
    "cols = [\"hidden_dim\", \"num_layers\", \"edge_hidden\", \"jk_mode\", \"pool\", \"aggr\",\n",
    "        \"dropout\", \"lr\", \"best_val_pr_auc\", \"test_pr_auc\", \"test_roc_auc\",\n",
    "        \"test_f1\", \"test_precision\", \"test_recall\",\n",
    "        \"n_params\", \"n_edge_params\", \"best_epoch\", \"wall_seconds\"]\n",
    "cols = [c for c in cols if c in df.columns]\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"ALL COMPLETED TRIALS (by test PR-AUC)\")\n",
    "print(\"=\"*90)\n",
    "print(df[cols].head(15).to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "best = df.iloc[0]\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"BEST NNConv: hidden={best['hidden_dim']} layers={best['num_layers']}\"\n",
    "      f\" edge_hidden={best['edge_hidden']} pool={best['pool']}\")\n",
    "print(f\"  Test PR-AUC: {best['test_pr_auc']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best['test_roc_auc']:.4f}  F1: {best['test_f1']:.4f}\")\n",
    "print(f\"  Prec: {best['test_precision']:.4f}  Rec: {best['test_recall']:.4f}\")\n",
    "print(f\"  Params: {best['n_params']:,.0f}  (edge net: {best['n_edge_params']:,.0f})\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "# Ablation comparison\n",
    "print(\"\\nABLATION COMPARISON:\")\n",
    "adf = pd.DataFrame(ablation_results)\n",
    "print(adf[[\"ablation\", \"best_val_pr_auc\", \"test_pr_auc\",\n",
    "           \"test_f1\", \"n_params\"]].to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Dense search best\n",
    "dense_best = dense_study.best_trial.user_attrs[\"result\"]\n",
    "print(f\"\\nDENSE FEATURE SEARCH ({DENSE_FEAT_DIM} features):\")\n",
    "dense_df = pd.DataFrame(dense_results).sort_values(\"test_pr_auc\", ascending=False)\n",
    "print(dense_df[[\"hidden_dim\",\"num_layers\",\"edge_hidden\",\"pool\",\"jk_mode\",\n",
    "                 \"best_val_pr_auc\",\"test_pr_auc\",\"test_f1\",\"n_params\"]\n",
    "              ].head(10).to_string(index=False, float_format=\"%.4f\"))\n",
    "print(f\"\\nBest dense: test PR-AUC={dense_best['test_pr_auc']:.4f}  \"\n",
    "      f\"params={dense_best['n_params']:,.0f}\")\n",
    "\n",
    "# Signal verification\n",
    "real_pr = study.best_trial.user_attrs[\"result\"][\"test_pr_auc\"]\n",
    "rand_pr = res_rand[\"test_pr_auc\"]\n",
    "dense_pr = dense_best[\"test_pr_auc\"]\n",
    "delta = real_pr - rand_pr\n",
    "print(f\"\\nEdge feature signal:\")\n",
    "print(f\"  All 95 vs random:  {real_pr - rand_pr:+.4f}\")\n",
    "print(f\"  Dense vs random:   {dense_pr - rand_pr:+.4f}\")\n",
    "print(f\"  All 95 vs dense:   {real_pr - dense_pr:+.4f}\")\n",
    "if real_pr - rand_pr > 0.01:\n",
    "    print(\"  -> Edge features carry meaningful signal\")\n",
    "if abs(real_pr - dense_pr) < 0.01:\n",
    "    print(\"  -> Sparse features add little; dense subset is sufficient\")\n",
    "\n",
    "# Full baseline comparison\n",
    "print(\"\\nFULL BASELINE COMPARISON:\")\n",
    "for nm, v in [(\"Random\", 0.023), (\"LogReg (Wb02)\", 0.154),\n",
    "              (\"GLASS (2024)\", 0.208), (\"SAGE tuned (Wb03)\", 0.4848),\n",
    "              (\"GATv2 tuned (Wb03)\", 0.4964),\n",
    "              (\"NNConv best\", float(best['test_pr_auc'])),\n",
    "              (\"NNConv random edges\", res_rand['test_pr_auc']),\n",
    "              (f\"NNConv dense ({DENSE_FEAT_DIM} feats)\", dense_best['test_pr_auc'])]:\n",
    "    print(f\"  {nm:30s} {v:.4f}  {'█' * int(v * 100)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268cb74",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e781c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = df.iloc[0].to_dict()\n",
    "print(\"Re-training best NNConv for checkpoint...\")\n",
    "res_best, state_best = train_and_evaluate_nnconv(\n",
    "    hidden_dim=int(best_cfg[\"hidden_dim\"]),\n",
    "    num_layers=int(best_cfg[\"num_layers\"]),\n",
    "    edge_hidden=int(best_cfg[\"edge_hidden\"]),\n",
    "    dropout=float(best_cfg[\"dropout\"]),\n",
    "    lr=float(best_cfg[\"lr\"]),\n",
    "    pool=best_cfg[\"pool\"],\n",
    "    jk_mode=best_cfg[\"jk_mode\"],\n",
    "    aggr=best_cfg[\"aggr\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": state_best,\n",
    "    \"config\": {k: best_cfg[k] for k in\n",
    "               [\"hidden_dim\",\"num_layers\",\"edge_hidden\",\"dropout\",\n",
    "                \"lr\",\"pool\",\"jk_mode\",\"aggr\"] if k in best_cfg},\n",
    "    \"metrics\": {k: res_best[k] for k in\n",
    "                [\"best_val_pr_auc\",\"test_pr_auc\",\"test_roc_auc\",\n",
    "                 \"test_f1\",\"test_precision\",\"test_recall\"]},\n",
    "    \"in_dim\": IN_DIM,\n",
    "    \"edge_feat_dim\": EDGE_FEAT_DIM,\n",
    "}, WB03C2_DIR / \"best_model.pt\")\n",
    "print(f\"Saved → {WB03C2_DIR / 'best_model.pt'}  PR-AUC={res_best['test_pr_auc']:.4f}\")\n",
    "\n",
    "\n",
    "# Also save best dense model if it beats the full model\n",
    "if dense_best[\"test_pr_auc\"] >= res_best[\"test_pr_auc\"] - 0.005:\n",
    "    print(\"\\nDense model competitive — saving dense checkpoint too...\")\n",
    "    dense_cfg = dense_study.best_trial.params\n",
    "    res_d, state_d = train_and_evaluate_nnconv(\n",
    "        hidden_dim=dense_cfg[\"hidden_dim\"],\n",
    "        num_layers=dense_cfg[\"num_layers\"],\n",
    "        edge_hidden=dense_cfg[\"edge_hidden\"],\n",
    "        dropout=dense_cfg[\"dropout\"],\n",
    "        lr=dense_cfg[\"lr\"],\n",
    "        pool=dense_cfg[\"pool\"],\n",
    "        jk_mode=dense_cfg[\"jk_mode\"],\n",
    "        aggr=dense_cfg[\"aggr\"],\n",
    "        datasets=(train_ds_dense, val_ds_dense, test_ds_dense),\n",
    "        edge_feat_dim=DENSE_FEAT_DIM,\n",
    "        verbose=True,\n",
    "    )\n",
    "    torch.save({\n",
    "        \"model_state_dict\": state_d,\n",
    "        \"config\": dense_cfg,\n",
    "        \"metrics\": {k: res_d[k] for k in\n",
    "                    [\"best_val_pr_auc\",\"test_pr_auc\",\"test_roc_auc\",\n",
    "                     \"test_f1\",\"test_precision\",\"test_recall\"]},\n",
    "        \"in_dim\": IN_DIM,\n",
    "        \"edge_feat_dim\": DENSE_FEAT_DIM,\n",
    "        \"dense_meta\": dense_meta,\n",
    "    }, WB03C2_DIR / \"best_model_dense.pt\")\n",
    "    print(f\"Saved → {WB03C2_DIR / 'best_model_dense.pt'}  \"\n",
    "          f\"PR-AUC={res_d['test_pr_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55cedd",
   "metadata": {},
   "source": [
    "## 9. Optuna Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "vals = [t.value for t in study.trials if t.value is not None]\n",
    "axes[0].plot(vals, \"o-\", ms=4, alpha=0.7)\n",
    "axes[0].axhline(study.best_value, color=\"red\", ls=\"--\", alpha=0.5,\n",
    "                label=f\"Best: {study.best_value:.4f}\")\n",
    "axes[0].axhline(0.4964, color=\"blue\", ls=\":\", alpha=0.5,\n",
    "                label=\"GATv2 Wb03: 0.4964\")\n",
    "axes[0].set(xlabel=\"Trial\", ylabel=\"Val PR-AUC\",\n",
    "            title=\"NNConv Optimisation History\")\n",
    "axes[0].legend()\n",
    "\n",
    "try:\n",
    "    imp = optuna.importance.get_param_importances(study)\n",
    "    ps = list(imp.keys())[:8]\n",
    "    axes[1].barh(ps[::-1], [imp[p] for p in ps[::-1]])\n",
    "    axes[1].set(xlabel=\"Importance\", title=\"Parameter Importance\")\n",
    "except Exception as e:\n",
    "    axes[1].text(0.5, 0.5, str(e), ha=\"center\", va=\"center\",\n",
    "                 transform=axes[1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(WB03C2_DIR / \"optuna_diagnostics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "save_fig(\"results/fig_01.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d1e7c",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "**Outputs saved to `results/wb03c2/`:**\n",
    "- `search_results.json` — all completed trials\n",
    "- `ablation_results.json` — real vs random edges comparison\n",
    "- `best_model.pt` — best NNConv checkpoint\n",
    "- `optuna_diagnostics.png`\n",
    "\n",
    "**Key findings:**\n",
    "- Whether edge features improve over node-only models\n",
    "- Signal verification via random-edge ablation\n",
    "- Comparison against Wb03 GATv2/SAGE baselines and published GLASS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}